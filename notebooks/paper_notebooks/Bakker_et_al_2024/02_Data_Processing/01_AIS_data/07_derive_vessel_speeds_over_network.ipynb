{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1ea632-f64d-4edf-8159-26be4b80f655",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b477eae-09d6-47c2-823c-4796b5557a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_gateway\n",
    "import dask.distributed\n",
    "\n",
    "import dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import movingpandas as mpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyproj\n",
    "import pytz\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "from shapely.ops import transform\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f09740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the path to load pre-processed ais data\n",
    "folder_name = '2022_PoR'\n",
    "path_name = 'abfs://ais/parquet/' + folder_name  \n",
    "\n",
    "#sets the path to load other local data\n",
    "current_directory = os.getcwd()\n",
    "path = current_directory.split(\"\\\\01_Data_Analysis\\\\02_AIS_data\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635aa19-8069-4ff0-8fac-05506e58eb8d",
   "metadata": {},
   "source": [
    "### Loads the access token (we use a SAS-token to protect the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c89bd-b6c9-4d85-8f10-78c9de1b1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for environmental variables for secrets (needs python-dotenv)\n",
    "# You can copy the  .env.example file and rename it to .env (one directory  up from the notebooks)\n",
    "# \n",
    "%load_ext dotenv\n",
    "%load_ext line_profiler\n",
    "# Load environment variables from the .env file 1 directory up\n",
    "%dotenv -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20c267-183f-4c3f-8da9-e85e8be22113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the environment variable from the  .env file\n",
    "sas_token = dotenv.dotenv_values()['AZURE_BLOB_SAS_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81e3a2-049a-4fad-9a2f-7289945cf8f7",
   "metadata": {},
   "source": [
    "### Creation of the cluster with high worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4059fe-8a9a-4a12-867e-8fe7413cc510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gateway = dask_gateway.Gateway()\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options.worker_memory = 64\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.adapt(minimum=1, maximum=100)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a99df3-fea1-4b71-bc28-f4372c49583f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8438e3c-5e38-4635-892a-6e4a5ae32ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def worker_setup(dask_worker: dask.distributed.Worker):\n",
    "    import os\n",
    "    os.system(\"pip install -q movingpandas\")  # or pip\n",
    "\n",
    "client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326332a3-7d20-4e37-9d65-1db8a1defeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_file('ais_to_fis/fis_network.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51b2e6-1c7a-407d-935e-e5bb1fd9bc03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = pathlib.Path('/home/jovyan/AIS/Rotterdam/ais_to_fis').resolve()\n",
    "sys.path.insert(0, str(src_path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a69b23",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\FG.pickle\", 'rb') as f:\n",
    "    FG = pickle.load(f)\n",
    "    \n",
    "with open(path+\"\\\\03_Simulation\\\\00_Input_data\\\\01_Geospatial_data\\\\network\\\\PoR_graph_with_information.pickle\", 'rb') as f:\n",
    "    FG_model = pickle.load(f)\n",
    "\n",
    "anchorage_areas = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\anchorage_areas.geojson\")\n",
    "separation_zones = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\separation_zones.geojson\")\n",
    "separation_boundaries = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\separation_boundaries.geojson\")\n",
    "turning_basins = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\turning_basins.geojson\")\n",
    "waterways = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\water.geojson\")\n",
    "coastline = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\coastline.geojson\")\n",
    "berths = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\Berths.geojson\")\n",
    "liquid_bulk_terminals = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\liquid_bulk_terminals.geojson\")\n",
    "container_terminals = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\container_terminals.geojson\")\n",
    "dry_bulk_terminals = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\dry_bulk_terminals.geojson\")\n",
    "\n",
    "berths = berths.set_index('index')\n",
    "berths.index.names = ['Name']\n",
    "anchorage_areas['name'] = anchorage_areas['seamark:name']\n",
    "anchorage_areas = anchorage_areas.set_index('name')\n",
    "anchorage_areas = anchorage_areas[['geometry']]\n",
    "\n",
    "separation_zones['geometry'] = [Polygon(geom) for geom in separation_zones['geometry']] \n",
    "anchorage_areas['geometry'] = [Polygon(geom) for geom in anchorage_areas['geometry']] \n",
    "turning_basins['geometry'] = [Polygon(geom) for geom in turning_basins['geometry']] \n",
    "\n",
    "water_color = 'lightblue'\n",
    "boundary_color = 'k'\n",
    "liquid_bulk_color = 'dodgerblue'\n",
    "container_color = 'blue'\n",
    "dry_bulk_color = 'midnightblue'\n",
    "terminal_color = 'lightgrey'\n",
    "\n",
    "Koole = liquid_bulk_terminals[['Koole Tankstorage Botlek' in name if isinstance(name,str) else False for name in liquid_bulk_terminals.name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corrections to the edges and nodes to be removed\n",
    "for node in ['offshore1','offshore2','offshore3','offshore4','offshore5','offshore6','anchorage']:\n",
    "    FG_model.remove_node(node)\n",
    "    \n",
    "remove_edge_and_nodes = [['8863594', 'B33440_B'],\n",
    "                         ['B33440_B', 'B33440_A'],\n",
    "                         ['8866170', 'B49209_A'],\n",
    "                         ['B45863_B','B45863_A'],\n",
    "                         ['22161426', 'B45863_B'],\n",
    "                         ['8866170', 'B45863_A'],\n",
    "                         ['B49209_A','B49209_B'],\n",
    "                         ['B15339_B','B15339_A'],\n",
    "                         ['8864161', 'B15339_A'],\n",
    "                         ['8864161', '8867024'],\n",
    "                         ['8867008', '8866992'],\n",
    "                         ['8861414', '8867008'],\n",
    "                         ['8861414', '8867154'],\n",
    "                         ['8863786', '8865027'],\n",
    "                         ['8867154', '8863786'],\n",
    "                         ['8867154', '8867029'],\n",
    "                         ['8867029', 'S38127_B'],\n",
    "                         ['S38127_A', 'S38127_B'],\n",
    "                         ['B44072_B','B44072_A'],\n",
    "                         ['S38127_A','B44072_B'],\n",
    "                         ['8863200', 'B44072_A'],\n",
    "                         ['8860947', '8867066'],\n",
    "                         ['8863139', '8863914'],\n",
    "                         ['8862288', '8862714'],\n",
    "                         ['B57361_B','B57361_A'],\n",
    "                         ['B57361_A', 'B7951_B'],\n",
    "                         ['8860647', '8864217']]\n",
    "\n",
    "remove_nodes = ['8868450','22161408','8862288','8863139','8864345','8865732','8867066','8868418','B44869_A','B14087_A','B7951_B','B7951_A','8866992']\n",
    "\n",
    "ignore_edges = [['8867547', '8862973'],['8860596', '8864988'],['8868239', '8867363'],['8861764', '8863206'],['8861718', '8866182'],['8867449', 'B5729_B'],['8862930', '8866083'],\n",
    "                ['8864805', '8867639'],['8864288', '8864805'],['8864288', '8865822']]\n",
    "\n",
    "add_ignore_edges = []\n",
    "for edge in ignore_edges:\n",
    "    add_ignore_edges.append([edge[1],edge[0]])\n",
    "ignore_edges.extend(add_ignore_edges)\n",
    "ignore_edges = [tuple(edge) for edge in ignore_edges]\n",
    "\n",
    "ignore_nodes = ['8862973','8864988','8867363','8861764','8861718','B5729_B','8866083','8864805', '8867639','8867547','8862214','8862925','S14716_B','S14716_A',\n",
    "                '8867547','8862585','8866260','8867341','8860701','B17838816_A','8862143','B42792_B','8867784']\n",
    "\n",
    "for edge in remove_edge_and_nodes:\n",
    "    if edge in FG_model.edges:\n",
    "        FG_model.remove_edge(edge[0],edge[1])\n",
    "    if edge[0] in FG_model.nodes:\n",
    "        FG_model.remove_node(edge[0])\n",
    "    if edge[1] in FG_model.nodes:\n",
    "        FG_model.remove_node(edge[1])\n",
    "        \n",
    "for node in remove_nodes:\n",
    "    if node in FG_model.nodes:\n",
    "        FG_model.remove_node(node)\n",
    "        \n",
    "non_intersection_nodes = []\n",
    "for node in FG_model.nodes:\n",
    "    if len(FG_model.edges(node)) == 2:\n",
    "        non_intersection_nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8cc3a",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('magma')\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=14)\n",
    "\n",
    "knots = 0.514444444\n",
    "wgs84_ = pyproj.CRS('EPSG:4326')\n",
    "utm = pyproj.CRS('EPSG:32631')\n",
    "wgs84_to_utm = pyproj.Transformer.from_crs(wgs84_, utm, always_xy=True).transform\n",
    "utm_to_wgs84 = pyproj.Transformer.from_crs(utm, wgs84_, always_xy=True).transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21528e-d6f9-4716-8f07-11fdc8037f1e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee0706-8366-4320-9451-eff6833da6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_rgb_transparent(rgb, bg_rgb,alpha):\n",
    "    \"\"\" \n",
    "    Function that creates a non-transparent color based on a transparent color\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rgb: RGB-code as a list of RGB numbers from 0.0 to 1.0 as floats\n",
    "    bg_rgb: background RGB-code as a list of RGB numbers from 0.0 to 1.0 as floats\n",
    "    alpha: transparency of the RGB-color on the background RGB-color from 0.0 to 1.0 as float\n",
    "    \n",
    "    :returns: non-transparent color\n",
    "    \"\"\"\n",
    "    \n",
    "    non_transparent_color = [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]\n",
    "    \n",
    "    return non_transparent_color\n",
    "\n",
    "def dataframe_preparation(df):\n",
    "    \"\"\" \n",
    "    Function that prepares the voyage dataframe for further analysis\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: voyage dataframe\n",
    "    \n",
    "    :returns: prepared voyage dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    non_transparent_color = [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]\n",
    "    \n",
    "    return non_transparent_color\n",
    "    new_df = pd.DataFrame(columns=['name','trip_id','traj_id','time','longitude','latitude','draught','sog','cog'])\n",
    "    for loc,row_info in df.iterrows():\n",
    "        length_df = len(row_info.coordinates)\n",
    "        names = [row_info['name']]*length_df\n",
    "        trip_id = [loc]*length_df\n",
    "        traj_id = [row_info['name']+'_'+str(loc)]*length_df\n",
    "        times = [datetime.datetime.fromtimestamp((time-np.datetime64(\"1970-01-01\"))/np.timedelta64(1,'s')) for time in row_info.times]\n",
    "        longitudes = [coord.x for coord in row_info.coordinates]\n",
    "        latitudes = [coord.y for coord in row_info.coordinates]\n",
    "        draught = row_info.draught\n",
    "        sog = row_info.sog\n",
    "        cog = row_info.cog\n",
    "        row_df = pd.DataFrame({'name':names,'trip_id':trip_id,'traj_id':traj_id,'time':times,'longitude':longitudes,'latitude':latitudes,'draught':draught,'sog':sog,'cog':cog})\n",
    "        new_df = pd.concat([new_df,row_df])\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "def reset_index(df):\n",
    "    \"\"\" \n",
    "    Function that resets the index and keeps the old index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    :returns: dataframe\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop = False)\n",
    "    return df\n",
    "\n",
    "def reorder_columns(df,columns):\n",
    "    \"\"\" \n",
    "    Function that reorders the columns of a pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "def calculate_trajectory_information(df):\n",
    "    \"\"\" \n",
    "    Function that calculates the time, distance, speed, and direction information of a trajectory dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    import movingpandas as mpd\n",
    "    columns=list(df.columns)\n",
    "    columns = columns + ['distance','timedelta','speed','direction','directiondelta']\n",
    "    new_df = pd.DataFrame(columns=columns)\n",
    "    gdf = gpd.GeoDataFrame(df,columns=columns,crs=\"EPSG:4326\",geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "    trajectories = mpd.TrajectoryCollection(gdf,traj_id_col='traj_id',t='time',x='longitude',y='latitude',crs=\"EPSG:4326\")\n",
    "    for trajectory in trajectories:\n",
    "        trajectory.add_distance(overwrite=True)\n",
    "        trajectory.add_timedelta(overwrite=True)\n",
    "        trajectory.add_speed(overwrite=True)\n",
    "        trajectory.add_direction(overwrite=True)\n",
    "    if trajectories:\n",
    "        new_df = trajectories.to_point_gdf()\n",
    "    new_df = new_df.reset_index(drop=False)\n",
    "    directiondelta = [0]\n",
    "    for phi1,phi2 in zip(new_df.direction.to_numpy()[:-1],new_df.direction.to_numpy()[1:]):\n",
    "        directiondelta.append(abs(phi2-phi1))\n",
    "    directiondelta = [ddir if ddir <= 180 else ddir-180 for ddir in directiondelta]\n",
    "    new_df['directiondelta'] = directiondelta\n",
    "    new_df.at[0,'timedelta'] = pd.Timedelta(0,'s')              \n",
    "    new_df = new_df[columns]\n",
    "    return new_df\n",
    "\n",
    "def add_checkpoints(df, thr_m=25, thr_dist=1000, thr_course=30):\n",
    "    \"\"\" \n",
    "    Function that sets checkpoints over the trajectory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    thr_m: time threshold in minutes\n",
    "    thr_dist: distance threshold in meters\n",
    "    thr_course: course threshold in degrees\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    cum_dist = 0\n",
    "    cum_course = 0\n",
    "    df['checkpt'] = 0\n",
    "    df = df.reset_index(drop = True)\n",
    "    for i in df.index[1:]:\n",
    "        cum_dist += df.at[i, 'distance']\n",
    "        cum_course += df.at[i, 'directiondelta']\n",
    "\n",
    "        # if start of new trip, add checkpoints (begin and end) and reset cumulatives \n",
    "        if (df.at[i, 'traj_id'] != df.at[i-1, 'traj_id']): \n",
    "            df.at[i-1, 'checkpt'] = 1\n",
    "            df.at[i, 'checkpt'] = 1\n",
    "            cum_dist = 0\n",
    "            cum_course = 0\n",
    "        \n",
    "        # if threshold is exceeded, add checkpoint and reset cumulatives \n",
    "        elif (cum_dist > thr_dist) | (abs(cum_course) > thr_course): \n",
    "            df.at[i, 'checkpt'] = 1\n",
    "            cum_dist = 0\n",
    "            cum_course = 0\n",
    "            \n",
    "    # add checkpoints for the first and last sample \n",
    "    df.at[0, 'checkpt'] = 1\n",
    "    df.at[df.index[-1], 'checkpt'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_neighbors(nbs, G): \n",
    "    \"\"\" \n",
    "    Function that finds the neighbouring nodes of a list of neighbouring nodes (connected by edges)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nbs: neighbouring nodes\n",
    "    G: networkx graph\n",
    "    \n",
    "    :returns: networkx graph\n",
    "    \"\"\"\n",
    "    \n",
    "    new_nbs = []\n",
    "    for nb in nbs: \n",
    "        new_nbs += [n for n in G.neighbors(nb)]\n",
    "\n",
    "    all_nbs = nbs + new_nbs\n",
    "    return all_nbs\n",
    "\n",
    "\n",
    "def get_x_deg_sg(nodes, x, G): \n",
    "    \"\"\" \n",
    "    Function that creates a networkx subgraph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes: list of node strings\n",
    "    x: number of nodes\n",
    "    G: networkx graph\n",
    "    \n",
    "    :returns: networkx graph\n",
    "    \"\"\"\n",
    "    if type(nodes) != list: \n",
    "        nodes = [nodes]\n",
    "\n",
    "    for i in list(range(0, x)): \n",
    "        nodes = get_neighbors(nodes, G)\n",
    "\n",
    "    subgraph = G.subgraph(nodes)\n",
    "    return subgraph \n",
    "\n",
    "def create_edge_paths(df, G): \n",
    "    \"\"\" \n",
    "    Function that creates paths of edges over the trajectory\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    G: networkx graph\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    import fis_network\n",
    "    \n",
    "    df['ch_cl_node'] = ''\n",
    "    df['ch_cl_edge'] = ''\n",
    "    df['path'] = ''\n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']), crs=\"EPSG:4326\")\n",
    "    \n",
    "    # create a graph that covers the AIS track\n",
    "    SG_ais = fis_network.reduce_FG_to_AIS_area(G, df)\n",
    "    \n",
    "    ind_cps = df[df['checkpt'] == 1].index \n",
    "\n",
    "    for i_cp, idx_cp in enumerate(ind_cps):\n",
    "        \n",
    "        # if the checkpoint is the first in a trip, reset the graph\n",
    "        if (i_cp == 0) | ((i_cp > 0) & (df.at[ind_cps[i_cp-1], 'traj_id'] != df.at[ind_cps[i_cp], 'traj_id'])):\n",
    "            SG = SG_ais\n",
    "\n",
    "            # determine closest node to checkpoint\n",
    "            curr_edge, curr_node = fis_network.find_closest_edge_and_node(SG, df.at[ind_cps[i_cp], 'geometry'])\n",
    "            df.at[ind_cps[i_cp], 'ch_cl_node'] = curr_node\n",
    "            df.at[ind_cps[i_cp], 'ch_cl_edge'] = curr_edge\n",
    "\n",
    "        else:\n",
    "            # derive a subgraph based on previous node\n",
    "            prev_node = df.at[ind_cps[i_cp-1], 'ch_cl_node']\n",
    "            SG = get_x_deg_sg(curr_node, 4, G)\n",
    "            # determine closest node to checkpoint\n",
    "            \n",
    "            curr_edge, curr_node = fis_network.find_closest_edge_and_node(SG, df.at[ind_cps[i_cp], 'geometry'])\n",
    "            df.at[ind_cps[i_cp], 'ch_cl_node'] = curr_node\n",
    "            df.at[ind_cps[i_cp], 'ch_cl_edge'] = curr_edge\n",
    "        \n",
    "    keep_node = list(dict.fromkeys([list(df.ch_cl_node.to_numpy()).index(node) for node in [path for path in df.ch_cl_node.to_numpy() if path != '']]))\n",
    "    if 0 not in keep_node:\n",
    "        keep_node.insert(0,0)\n",
    "    if len(df)-1 not in keep_node:\n",
    "        keep_node.append(len(df)-1)\n",
    "        \n",
    "    paths = [edge for idx,edge in df['ch_cl_edge'].items() if idx in keep_node]\n",
    "    routes = []\n",
    "    all_routes = None\n",
    "    for idx,(path1,path2) in enumerate(zip(paths[:-1],paths[1:])):\n",
    "        node1,node2 = path1[0],path1[1]\n",
    "        node3,node4 = path2[0],path2[1]\n",
    "        route1 = nx.dijkstra_path(FG,node1,node3)\n",
    "        route2 = nx.dijkstra_path(FG,node1,node4)\n",
    "        route3 = nx.dijkstra_path(FG,node2,node3)\n",
    "        route4 = nx.dijkstra_path(FG,node2,node4)\n",
    "        all_routes = [route1,route2,route3,route4]\n",
    "        route_index = np.argmax([len(route) for route in all_routes])\n",
    "        routes[keep_node[idx]:keep_node[idx+1]] = [all_routes[route_index]]*(keep_node[idx+1]-keep_node[idx])\n",
    "    routes.append(all_routes[route_index])\n",
    "    df['path'] = routes\n",
    "\n",
    "    df.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def df_with_closest_edges(df, G): \n",
    "    \"\"\" \n",
    "    Function that add the checkpoint, closest node, closest edge, path and geometry to the dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    G: networkx graph\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    import fis_network\n",
    "    df['cl_edge'] = ''\n",
    "    \n",
    "    # add geometry to dataframe\n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']), crs=\"EPSG:4326\")\n",
    "    \n",
    "    for idx,info in df.iterrows():\n",
    "        cl_edge, _ = fis_network.find_closest_edge(G.subgraph(info.path), point=Point([info.longitude,info.latitude]))\n",
    "        node1,node2 = cl_edge[0],cl_edge[1]\n",
    "        if list(info.path).index(node1) > list(info.path).index(node2):\n",
    "            node1,node2 = node2,node1\n",
    "        df.at[idx, 'cl_edge'] = (str(node1),str(node2))\n",
    "    \n",
    "    df = df.drop(['ch_cl_edge', 'ch_cl_node', 'checkpt','path','geometry'],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_row_edge_crossing(df_ais, edge_ixs, colnames):\n",
    "    \"\"\" \n",
    "    Function that creates a row for the edge dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_ais: pandas dataframe\n",
    "    edge_ixs: loc in pandas dataframe\n",
    "    colnames: column names as a list of strings\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df_row = pd.DataFrame(columns=colnames, index=[0])\n",
    "    df_row.loc[0, 'edgeA'] = df_ais.loc[edge_ixs[0], 'cl_edge'][0]\n",
    "    df_row.loc[0, 'edgeB'] = df_ais.loc[edge_ixs[0], 'cl_edge'][1]\n",
    "    df_row.loc[0, 'vesselname'] = df_ais.loc[edge_ixs[0], 'name']\n",
    "    df_row.loc[0, 'tripnr'] = df_ais.loc[edge_ixs[0], 'traj_id']\n",
    "    \n",
    "    df_row.loc[0, 't_start'] = df_ais.loc[edge_ixs[0], 'time']\n",
    "    df_row.loc[0, 'draught'] = df_ais['draught'].loc[edge_ixs].max()\n",
    "    df_row.loc[0, 'edge_duration'] = df_ais['timedelta'].loc[edge_ixs].sum()\n",
    "    df_row.loc[0, 'edge_dist'] = df_ais['distance'].loc[edge_ixs].sum()\n",
    "    if not pd.isnull(df_row.loc[0, 'edge_duration']) and df_row.loc[0, 'edge_duration'] > pd.Timedelta(0,'s'):\n",
    "        df_row.loc[0, 'edge_speed'] = df_row.loc[0, 'edge_dist'] / df_row.loc[0, 'edge_duration'].total_seconds()\n",
    "    else:\n",
    "        df_row.loc[0, 'edge_speed'] = 0\n",
    "\n",
    "    df_row.loc[[0], 'dtime'] = pd.Series([list(df_ais.loc[edge_ixs, 'timedelta'])],index=[0])\n",
    "    df_row.loc[[0], 'ddist'] = pd.Series([list(df_ais.loc[edge_ixs, 'distance'])],index=[0])\n",
    "    df_row.loc[[0], 'latlon'] = pd.Series([[list(df_ais.loc[edge_ixs, 'latitude']),\n",
    "                               list(df_ais.loc[edge_ixs, 'longitude'])]],index=[0])\n",
    "\n",
    "    return df_row\n",
    "\n",
    "def create_df_edges(df):\n",
    "    \"\"\" \n",
    "    Function that creates an edge dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    edge_ixs = [df.index[0]]\n",
    "    cols = ['edgeA', 'edgeB', 'vesselname', 'traj_id', 'draught', 't_start', 'edge_duration', 'edge_dist', 'edge_speed', 'dtime', 'ddist', 'latlon', 'clus']\n",
    "    \n",
    "    df_edges = pd.DataFrame(columns=cols)\n",
    "    for i, ix in enumerate(df.index[:-1]):\n",
    "        curr_edge = (df['cl_edge'].iloc[i][0], df['cl_edge'].iloc[i][1])\n",
    "        next_edge = (df['cl_edge'].iloc[i+1][0], df['cl_edge'].iloc[i+1][1])\n",
    "        \n",
    "        curr_trip = df['traj_id'].iloc[i]\n",
    "        next_trip = df['traj_id'].iloc[i+1]\n",
    "        \n",
    "        if (curr_edge == next_edge) & (curr_trip == next_trip):\n",
    "            edge_ixs.append(df.index[i + 1])\n",
    "            \n",
    "        else:\n",
    "            # create row for edge crossing\n",
    "            df_row = create_row_edge_crossing(df, edge_ixs, cols)\n",
    "            df_edges = pd.concat([df_edges, df_row])\n",
    "            edge_ixs = [df.index[i + 1]]\n",
    "\n",
    "    df_row = create_row_edge_crossing(df, edge_ixs, cols)\n",
    "    df_edges = pd.concat([df_edges, df_row])\n",
    "    df_edges = df_edges.reset_index(drop=True)\n",
    "    return df_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd0e54-ceb6-4d63-8446-255bd320e328",
   "metadata": {},
   "source": [
    "## Import results AIS data analysis and open geospatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af74d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/ship_dataframe',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ship_dataframe = ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f5d2b-6e4e-47fe-9f7e-ce7ed088b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/voyage_dataframe',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "geometry_columns = ['geometry_anchorage','geometry_entry','geometry_departure','coordinates']\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(renumber_index)\n",
    "ddf_i = ddf_i.map_partitions(convert_string_geometry_to_shapely_geometry,geometry_columns)\n",
    "voyage_dataframe = ddf_i.compute()\n",
    "voyage_dataframe = voyage_dataframe.rename(columns={'trip_number':'trip_id'})\n",
    "voyage_dataframe = voyage_dataframe[voyage_dataframe.berth_of_call.isin(berths.index)]\n",
    "voyage_dataframe = voyage_dataframe.reset_index(drop=True)\n",
    "voyage_dataframe = dataframe_preparation(voyage_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc70d8b-c469-4a72-81df-d9184b5e2dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dask.dataframe.from_pandas(voyage_dataframe,npartitions=1)\n",
    "ddf = ddf.set_index('traj_id',npartitions=len(list(set(df.traj_id))))\n",
    "scheme_information = {'trip_id': pa.int64()}\n",
    "ddf = ddf.map_partitions(reset_index)\n",
    "ddf = ddf.map_partitions(reorder_columns,columns=df.columns)\n",
    "ddf.to_parquet(path_name+'/ais_tracks', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow',write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021f21c-bdca-4582-a35a-5bd4566c1233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/ais_tracks', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f8378-e9ee-49b9-a045-a16ba5f84a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf_i = ddf.partitions[:]\n",
    "df_v = pd.DataFrame(columns=['name','trip_id','traj_id','time','longitude','latitude','draught','sog','cog','distance','timedelta','speed','direction','directiondelta'])\n",
    "ddf_i = ddf_i.map_partitions(calculate_trajectory_information,meta=df_v)\n",
    "df_cp = pd.DataFrame(columns=['name','trip_id','traj_id','time','longitude','latitude','draught','sog','cog','distance','timedelta','speed','direction','directiondelta','checkpt'])\n",
    "ddf_i = ddf_i.map_partitions(add_checkpoints,meta=df_cp)\n",
    "df_p = pd.DataFrame(columns=['name','trip_id','traj_id','time','longitude','latitude','draught','sog','cog','distance','timedelta','speed','direction','directiondelta','checkpt','ch_cl_node','ch_cl_edge','path'])\n",
    "ddf_i = ddf_i.map_partitions(create_edge_paths,G=FG,meta=df_p)\n",
    "df_ais = pd.DataFrame(columns=['name','trip_id','traj_id','time','longitude','latitude','draught','sog','cog','distance','timedelta','speed','direction','directiondelta','cl_edge'])\n",
    "ddf_i = ddf_i.map_partitions(df_with_closest_edges,G=FG,meta=df_ais)\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'trip_id':pa.int64(),\n",
    "                      'traj_id': pa.string(),\n",
    "                      'time': pa.timestamp('ns'),\n",
    "                      'longitude': pa.float64(),\n",
    "                      'latitude': pa.float64(),\n",
    "                      'draught':pa.float64(),\n",
    "                      'sog': pa.float64(),\n",
    "                      'cog': pa.float64(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'timedelta': pa.duration('ns'),\n",
    "                      'speed': pa.float64(),\n",
    "                      'direction': pa.float64(),\n",
    "                      'directiondelta': pa.float64(),\n",
    "                      'cl_edge': pa.list_(pa.string())}\n",
    "for datacolumn,datatype in scheme_information.items(): \n",
    "    ddf_i[datacolumn] = ddf_i[datacolumn].astype(datatype.to_pandas_dtype())\n",
    "ddf_i = ddf_i.repartition(npartitions=1)\n",
    "ddf_i.to_parquet(path_name+'/edge_dataframe', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow',write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a14bf-7ade-48c2-a6bb-b9456608987f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/edge_dataframe', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "df_ais = ddf.compute()\n",
    "df_ais = df_ais.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf466e-726a-4eb6-8d84-5789b2e2add4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_edges = create_df_edges(df_ais)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa8b09-736a-4432-a556-d1f995bdf9d8",
   "metadata": {},
   "source": [
    "### Group the data by edge name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8d511-e865-4700-b4ef-720dc3d9f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create overview of edges\n",
    "df_edges['edge'] = list(zip(df_edges['edgeA'], df_edges['edgeB']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571750ba-1fa3-4dda-837b-71c970720bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edge = df_edges.groupby(['edge']).agg({'vesselname': list, 'edge_duration': list, 'edge_dist': list, 'edge_speed': list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1535f3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Figure 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06b8fe-b3ea-47ce-aaae-442d9c3416f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=[32,18])\n",
    "lon_min = 3.9\n",
    "lon_max = 4.385\n",
    "lat_min = 51.835\n",
    "lat_max = 52.015\n",
    "ax.set_facecolor('lightgrey')\n",
    "water_color = 'lightblue'\n",
    "boundary_color = 'k'\n",
    "color_1 = (135/255,135/255,135/255) #'dodgerblue'\n",
    "color_2 = (135/255,135/255,135/255) #'blue'\n",
    "color_3 = (135/255,135/255,135/255) #'midnightblue'\n",
    "anchorage_color = 'limegreen'\n",
    "seperation_color = 'violet'\n",
    "terminal_color = 'darkgrey'\n",
    "\n",
    "color_calandlijn = (226/255,33/255,18/255)\n",
    "color_erasmuslijn = (2/255,58/255,141/255)\n",
    "\n",
    "waterways.plot(ax=ax,facecolor=water_color,edgecolor='none',linewidth=2,zorder=100)\n",
    "ax.fill([0,0,0,0],[0,0,0,0],color=water_color,label='water')\n",
    "\n",
    "x0 = FG.nodes['8866969']['geometry'].x\n",
    "y0 = FG.nodes['8866969']['geometry'].y+0.004\n",
    "x2 = np.mean([FG.nodes['8866969']['geometry'].x,FG.nodes['8866305']['geometry'].x])\n",
    "y2 = np.mean([FG.nodes['8866969']['geometry'].y,FG.nodes['8866305']['geometry'].y])+0.004\n",
    "x1 = x0-(x2-x0)\n",
    "y1 = y0-(y2-y0)\n",
    "angle = np.arctan2(x2-x0,y2-y0)+0.05*np.pi\n",
    "dist = np.sqrt((x2-x0)**2+(y2-y0)**2)/1.25\n",
    "x3 = x2+np.sin(angle-0.5*np.pi)*dist\n",
    "y3 = y2+np.cos(angle-0.5*np.pi)*dist\n",
    "x4 = x1+np.sin(angle-0.5*np.pi)*dist\n",
    "y4 = y1+np.cos(angle-0.5*np.pi)*dist\n",
    "virtual_anchorage_area = Polygon([Point(x1,y1),Point(x2,y2),Point(x3,y3),Point(x4,y4)]).exterior\n",
    "ax.fill([coord[0] for coord in virtual_anchorage_area.coords],\n",
    "        [coord[1] for coord in virtual_anchorage_area.coords],\n",
    "        facecolor='none',edgecolor=anchorage_color,linestyle='--',label='anchorage area',linewidth=3,zorder=100)\n",
    "\n",
    "Koole = Koole.reset_index(drop=True)\n",
    "new_Koole = Koole.copy()\n",
    "for loc,info in Koole.iterrows():\n",
    "    new_Koole.loc[loc,'geometry'] = info['geometry']\n",
    "new_Koole.plot(ax=ax,color=terminal_color,edgecolor='none',linewidth=5,zorder=99)\n",
    "\n",
    "# turning_basins.plot(ax=ax,facecolor='none',edgecolor='k',linewidth=2,linestyle='--',zorder=100)\n",
    "# ax.plot([0,0], [0,0], marker='$\\u25CC$', markerfacecolor='k', markeredgecolor='none', markersize=32, linestyle='none',linewidth=1,label='turning basin')\n",
    "\n",
    "for node in FG_model.nodes:\n",
    "    coords = FG_model.nodes[node]['geometry'].coords.xy\n",
    "    ax.scatter(coords[0],coords[1],s=30,linewidth=3,color=make_rgb_transparent(color_erasmuslijn,mpl.colors.to_rgb(water_color),alpha=0.25),zorder=102)\n",
    "for edge in FG_model.edges:\n",
    "    coords = FG_model.edges[edge]['geometry'].coords.xy\n",
    "    ax.plot(*coords,linewidth=4,color=make_rgb_transparent(color_erasmuslijn,mpl.colors.to_rgb(water_color),alpha=0.25),zorder=102)\n",
    "\n",
    "edges_of_interest = pd.DataFrame(columns = df_edge.columns)\n",
    "for edge in df_edge.index:\n",
    "    if edge in FG_model.edges and edge not in ignore_edges:\n",
    "        edges_of_interest = pd.concat([edges_of_interest,df_edge.loc[[edge]]])\n",
    "        \n",
    "nodes_of_interest = []\n",
    "for edge in edges_of_interest.index:\n",
    "    if edge[0] not in nodes_of_interest:\n",
    "        nodes_of_interest.append(edge[0])\n",
    "    if edge[1] not in nodes_of_interest:\n",
    "        nodes_of_interest.append(edge[1])\n",
    "        \n",
    "for edge in edges_of_interest.index:\n",
    "    geom = FG_model.edges[edge]['geometry']\n",
    "    geom_p1 = FG_model.nodes[edge[0]]['geometry']\n",
    "    geom_p2 = FG_model.nodes[edge[1]]['geometry']\n",
    "    first = Point(geom.coords[0])\n",
    "    second = Point(geom.coords[-1])\n",
    "    angle = math.atan2((second.x-first.x),(second.y-first.y))\n",
    "    angle = angle/(2*np.pi)*360\n",
    "    geom = transform(wgs84_to_utm,geom)\n",
    "\n",
    "    if not edge[0] in list(FG_model.nodes) or not edge[1] in list(FG_model.nodes):\n",
    "        continue\n",
    "    route = nx.dijkstra_path(FG_model,edge[0],'8866969')\n",
    "    if edge[0] == '8866969':\n",
    "        if edge[1] == 'offshore6':\n",
    "            bound = 'outbound'\n",
    "        elif edge[1] == '8866305':\n",
    "            bound = 'inbound'\n",
    "            \n",
    "    if 'offshore6' in route:\n",
    "        if edge[1] in route and list(route).index(edge[1]) > list(route).index(edge[0]):\n",
    "            bound = 'inbound'\n",
    "        else:\n",
    "            bound = 'outbound'\n",
    "    elif '8866305' in route:\n",
    "        if edge[1] in route and list(route).index(edge[1]) < list(route).index(edge[0]):\n",
    "            bound = 'outbound'\n",
    "        else:\n",
    "            bound = 'inbound'\n",
    "\n",
    "    if bound == 'inbound':\n",
    "        if np.round(first.x,3) == np.round(geom_p1.x,3) and np.round(first.y,3) == np.round(geom_p1.y,3):\n",
    "            geom_final = geom.offset_curve(-100)\n",
    "        else:\n",
    "            geom_final = geom.offset_curve(100)\n",
    "    else:\n",
    "        if np.round(first.x,3) == np.round(geom_p1.x,3) and np.round(first.y,3) == np.round(geom_p1.y,3):\n",
    "            geom_final = geom.offset_curve(-100)\n",
    "        else:\n",
    "            geom_final = geom.offset_curve(100)\n",
    "\n",
    "    geom = transform(utm_to_wgs84,geom_final)\n",
    "    coords = geom.coords.xy\n",
    "    color = cmap(np.mean(edges_of_interest[edges_of_interest.index == (edge[0],edge[1])].edge_speed.iloc[0])/knots/14)\n",
    "    ax.plot(coords[0],coords[1],linewidth=6,color=color,zorder=102)\n",
    "\n",
    "ax.plot([0,0],[0,0],color=make_rgb_transparent(color_erasmuslijn,mpl.colors.to_rgb(water_color),alpha=0.25),linewidth=4,marker='o',markersize=8,label='Network')    \n",
    "ax.plot([0,0],[0,0],color=color_erasmuslijn,linewidth=8,marker='o',markerfacecolor='white',markeredgecolor='k',markersize=14,label='Routes',markeredgewidth=4)\n",
    "        \n",
    "nodes_x = []\n",
    "nodes_y = []\n",
    "for node in nodes_of_interest:\n",
    "    if node not in ignore_nodes:\n",
    "        coords = FG_model.nodes[node]['geometry'].coords.xy\n",
    "        nodes_x.append(coords[0])\n",
    "        nodes_y.append(coords[1])\n",
    "ax.scatter(nodes_x,nodes_y,s=160,linewidth=4,edgecolor='k',facecolor='white',zorder=102)    \n",
    "ax.set_xlim([lon_min,lon_max])\n",
    "ax.set_ylim([lat_min,lat_max]);\n",
    "\n",
    "legend_entities,labels = ax.get_legend_handles_labels()\n",
    "patches = [legend_entities[0],\n",
    "           mpl.patches.Patch(facecolor='lightgrey',edgecolor='none',linewidth=2),\n",
    "           mpl.patches.Patch(facecolor=color_1,edgecolor='none',linewidth=5),\n",
    "           legend_entities[1],\n",
    "           legend_entities[-2],\n",
    "           legend_entities[-1]]\n",
    "labels = ['Water','Land','Liquid bulk terminal','Virtual anchorage area','Network','Routes']\n",
    "ax.legend(patches,labels,loc='upper right',prop={'size': 25})\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.tick_params(bottom=False, labelbottom=False,\n",
    "               left=False, labelleft=False)\n",
    "cbar = plt.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),shrink=0.86,ax=ax,anchor=(-0.325,0.5025));\n",
    "cbar.outline.set_linewidth(2)\n",
    "cbar.ax.tick_params(labelsize=27)\n",
    "cbar.ax.set_ylabel('Speed [kn]',size=27);\n",
    "fig.savefig(path+'\\\\04_Output_data\\\\02_Figures\\\\Figure_06_Vessel_Speed_Network.eps', format='eps', dpi=1000,bbox_inches='tight',facecolor='none'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2c649",
   "metadata": {},
   "source": [
    "## Create vessel speed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ff607-cf90-4218-b2b7-27640cb29e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_edge['average_speed'] = [np.mean(speeds) for speeds in df_edge['edge_speed'].to_numpy()]\n",
    "vessel_speed_dataframe = df_edge[['average_speed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035af1c-01eb-4ea7-9df1-9d3ad582918b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path+'\\\\03_Simulation\\\\01_Input_data\\\\03_Vessels\\\\vessel_speed_dataframe.pickle', 'wb') as handle:\n",
    "    pickle.dump(vessel_speed_dataframe, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77de3f-e3a5-47c0-b28b-81c0e537b59a",
   "metadata": {},
   "source": [
    "## Origin-Destination Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a26c3-146f-4fe0-b68a-4db186781a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_origin_destination = voyage_dataframe[['name','trip_number','berth_of_call','coordinates','arrival_at_port','arrival_at_anchorage_at_arrival','departure_from_anchorage_at_arrival','arrival_at_port_entrance','departure_from_port_entrance','arrival_at_berth','departure_from_berth','arrival_at_turning_basin','departure_from_turning_basin','draught_at_arrival','draught_at_departure']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a63323-18b5-442a-bcc8-cef9f53b5d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrival_times = []\n",
    "for loc,info in df_origin_destination.iterrows():\n",
    "    if type(info.arrival_at_port_entrance) == float:\n",
    "        arrival_times.append(info.arrival_at_port)\n",
    "    elif pd.isnull(info.waiting_time_in_anchorage):\n",
    "        arrival_times.append(info.arrival_at_port_entrance)\n",
    "    else:\n",
    "        arrival_times.append(info.arrival_at_port_entrance-info.waiting_time_in_anchorage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8733e4e-4121-4b02-987b-3c5ae675aa23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_origin_destination['origin'] = [coordinates[0] for coordinates in df_origin_destination.coordinates.to_numpy()]\n",
    "df_origin_destination['destination'] = [coordinates[-1] for coordinates in df_origin_destination.coordinates.to_numpy()]\n",
    "df_origin_destination['waiting_time_in_anchorage'] = [time2-time1 if not type(time1) == float else pd.Timedelta(0,'s') for time1,time2 in zip(df_origin_destination.arrival_at_anchorage_at_arrival,df_origin_destination.departure_from_anchorage_at_arrival)]\n",
    "df_origin_destination['origin_node'] = [fis_network.find_closest_edge_and_node(FG,origin)[1] for origin in df_origin_destination.origin.to_numpy()]\n",
    "df_origin_destination['destination_node'] = [fis_network.find_closest_edge_and_node(FG,destination)[1] for destination in df_origin_destination.destination.to_numpy()]\n",
    "df_origin_destination['berth_node'] = '8866999'\n",
    "df_origin_destination['origin_node'] = ['8866969' if not type(arrival) == float else origin for origin,arrival in zip(df_origin_destination.origin_node.to_numpy(),df_origin_destination.arrival_at_port_entrance.to_numpy())]\n",
    "df_origin_destination['destination_node'] = ['8866969' if not type(departure) == float else destination for destination,departure in zip(df_origin_destination.destination_node.to_numpy(),df_origin_destination.departure_from_port_entrance.to_numpy())]\n",
    "df_origin_destination['turning_time'] = [time2-time1 if not type(time1) == float else pd.Timedelta(0,'s') for time1,time2 in zip(df_origin_destination.arrival_at_turning_basin,df_origin_destination.departure_from_turning_basin)]\n",
    "df_origin_destination['arrival'] = [time.tz_localize(pytz.timezone('UTC')) if not time.tz else time for time in arrival_times]\n",
    "df_origin_destination['draught'] = df_origin_destination['draught_at_arrival'] \n",
    "df_origin_destination['(un)loading'] = df_origin_destination['draught_at_arrival']-df_origin_destination['draught_at_departure'] \n",
    "df_origin_destination['length'] = [ship_dataframe.loc[name].length for name in df_origin_destination.name.to_numpy()]\n",
    "df_origin_destination['width'] = [ship_dataframe.loc[name].width for name in df_origin_destination.name.to_numpy()]\n",
    "df_origin_destination['(un)loading time'] = [time2-time1 if type(time2) != float else pd.Timestamp('2020-01-01',tz='UTC')-time1 for time1,time2 in zip(df_origin_destination['arrival_at_berth'].to_numpy(),df_trips['departure_from_berth'].to_numpy())]\n",
    "df_origin_destination['trip_id'] = df_origin_destination['name']+'_'+df_origin_destination['trip_number'].astype(str)\n",
    "df_origin_destination = df_origin_destination[['name','trip_id','length','width','draught','(un)loading','berth_of_call','arrival','waiting_time_in_anchorage','turning_time','(un)loading time','origin_node','berth_node','destination_node']]\n",
    "df_origin_destination['arrival'] = [pd.Timestamp(time.strftime('%Y-%m-%d %X'),unit='s',tz=pytz.timezone('UTC')) for time in df_origin_destination.arrival.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d74548-9950-4569-8871-0778b9fd3a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(path+'\\\\03_Simulation\\\\01_Input_data\\\\03_Vessels\\\\origin_destination_PoR.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_origin_destination, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6f0f1-cb3f-4113-a40f-eb841b6bb340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
