{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427adf97-36cc-47c5-a678-009dca2d3b2e",
   "metadata": {},
   "source": [
    "### Import packages and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9012d-0e9f-4e91-878b-0f7846fe26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_gateway\n",
    "import dask.distributed\n",
    "\n",
    "import dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon, LineString, Point, MultiPolygon\n",
    "from shapely.ops import transform, cascaded_union\n",
    "import numpy as np\n",
    "import movingpandas as mpd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pyproj\n",
    "import scipy\n",
    "import pyarrow as pa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f60dd-d925-468d-9307-4eda697ef5d7",
   "metadata": {},
   "source": [
    "Set the path to the AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444fee6-d2b6-4261-b160-c3af08fba927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the path to load pre-processed ais data\n",
    "folder_name = '2022_PoR'\n",
    "path_name = 'abfs://ais/parquet/' + folder_name  \n",
    "\n",
    "#sets the path to load other local data\n",
    "current_directory = os.getcwd()\n",
    "path = current_directory.split(\"\\\\01_Data_Analysis\\\\02_AIS_data\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8c67c-eee7-410e-9a4d-977942a60a77",
   "metadata": {},
   "source": [
    "### Loads the access token (we use a SAS-token to protect the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1d5ce-ef28-4769-bbc0-cbbaa11491b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for environmental variables for secrets (needs python-dotenv)\n",
    "# You can copy the  .env.example file and rename it to .env (one directory  up from the notebooks)\n",
    "# \n",
    "%load_ext dotenv\n",
    "# Load environment variables from the .env file 1 directory up\n",
    "%dotenv -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fed6d-5b22-4bdc-a902-bd540eed0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the environment variable from the  .env file\n",
    "sas_token = dotenv.dotenv_values()['AZURE_BLOB_SAS_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f0458-17fa-430f-a403-6db462db421e",
   "metadata": {},
   "source": [
    "### Creation of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccf28f-22dd-42a1-8753-c9212b574cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the calculation cluster\n",
    "gateway = dask_gateway.Gateway()\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.adapt(minimum=1, maximum=100)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bec2c3-fb90-4866-b00a-fcb59cb2218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provides access to the calculation cluster\n",
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f025840-9ccd-4f1f-a02c-cb5525644ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads packages to the workers of the calculation cluster\n",
    "def worker_setup(dask_worker: dask.distributed.Worker):\n",
    "    import os\n",
    "    os.system(\"pip install -q movingpandas\")  # or pip\n",
    "    os.system(\"pip install -q more-itertools\")\n",
    "    os.system(\"pip install -q dask\")\n",
    "\n",
    "client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b6763-3299-40bd-90c4-01ce9d795707",
   "metadata": {},
   "source": [
    "### Import geospatial data and creation of areas of interests and vessel types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd591b0-63d4-4f24-b0c5-744f2f98a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates transformation functions between spatial references systems\n",
    "utm = pyproj.CRS('EPSG:28992')\n",
    "wgs84 = pyproj.CRS('EPSG:4326')\n",
    "wgs_to_utm = pyproj.Transformer.from_crs(wgs84,utm,always_xy=True).transform\n",
    "utm_to_wgs = pyproj.Transformer.from_crs(utm,wgs84,always_xy=True).transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996e0cf-e28e-46fc-9032-c10813e89719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dictionary with the geospatial areas of interest\n",
    "anchorage_areas = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\anchorage_areas.geojson\")\n",
    "\n",
    "anchorage_areas['geometry'] = [Polygon(geom) for geom in anchorage_areas['geometry']] \n",
    "areas_of_interest = {}\n",
    "areas_of_interest['port_entrance'] = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\Port_Entrance.geojson\")['geometry'][0]\n",
    "areas_of_interest['berths'] = transform(utm_to_wgs,MultiPolygon(pickle.load(open(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\berths_PoR.pickle\",'rb'))['geometry'].to_list()))\n",
    "areas_of_interest['anchorage_areas'] = cascaded_union(anchorage_areas['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09441758-dd32-4299-bb82-700f6be8d138",
   "metadata": {},
   "source": [
    "### Create and select trajectories of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e686d-6bcf-42d7-b800-b560fcd5f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def create_sorting_key(df,columns):\n",
    "    \"\"\" \n",
    "    Function that combines columns to create a sorting key \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    columns: list of the column names of the dataframe of which the sorting keys should be composed of\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    for column_index,column_name in enumerate(columns):\n",
    "        if column_index == 0:\n",
    "            sorting_key = df[column_name].astype(str)\n",
    "            continue\n",
    "        sorting_key = sorting_key + '_' + df[column_name].astype(str)\n",
    "    df['sorting_key'] = sorting_key\n",
    "    return df\n",
    "\n",
    "def add_columns(df,added_columns):\n",
    "    \"\"\" \n",
    "    Function that adds new columns with NaN floats to the dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    add_columns: list of names that should be new columns\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    for added_column in added_columns:\n",
    "        df[added_column] = np.NaN\n",
    "    return df\n",
    "\n",
    "def create_gdf(df):\n",
    "    \"\"\" \n",
    "    Function that creates a geopandas dataframe from a pandas dataframe with ['longitude'] and ['latitude']\n",
    "    columns with values in WGS4326\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "\n",
    "    :returns:  geopandas dataframe\n",
    "    \"\"\"\n",
    "    gdf = geopandas.GeoDataFrame(df,columns=df.columns,crs=\"EPSG:4326\",geometry=geopandas.points_from_xy(df.longitude, df.latitude))\n",
    "    return gdf\n",
    "\n",
    "def transform_projection(gdf,crs):\n",
    "    \"\"\" \n",
    "    Function that transforms the geometries of a geopandas dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geopandas dataframe\n",
    "    crs: coordinate reference system as a string in a 'EPSG:#'-format\n",
    "\n",
    "    :returns:  geopandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    transformed_gdf = gdf.to_crs(crs)\n",
    "    return transformed_gdf\n",
    "\n",
    "def trajectorize(gdf):\n",
    "    \"\"\" \n",
    "    Function that trajectorizes the AIS data in a geopandas dataframe using the following columns: ['name',\n",
    "    'timestamplast','latitude','longitude']\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geopandas dataframe\n",
    "\n",
    "    :returns:  movingpandas trajectory collection\n",
    "    \"\"\"\n",
    "    import movingpandas as mpd\n",
    "    traj_collection = mpd.TrajectoryCollection(gdf,traj_id_col='name',t='timestamplast',x='latitude',y='longitude')\n",
    "    traj_collection.add_speed(overwrite=True)\n",
    "    traj_collection.add_direction(overwrite=True)\n",
    "    traj_collection.add_acceleration(overwrite=True)\n",
    "    return traj_collection\n",
    "\n",
    "def splitter(traj_collection,max_diameter,min_duration,min_length,gap):\n",
    "    \"\"\" \n",
    "    Function that splits the movingpandas trajectories based on a time gap and a stop criterion\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj_collection: movingpandas trajectory collection\n",
    "    max_diameter: diameter in meters that holds as a boundary box for the stop condition\n",
    "    min_duration: minimum duration as pandas timedelta that holds as a timeframe for the stop condition\n",
    "    min_length: length in meters which a splitted trajectory should have in order to be stored in the collection\n",
    "    gap: time gap in as pandas timedelta\n",
    "\n",
    "    :returns:  movingpandas trajectory collection\n",
    "    \"\"\"\n",
    "    import movingpandas as mpd\n",
    "    splitted_trajs = mpd.ObservationGapSplitter(traj_collection).split(gap=gap)\n",
    "    splitted_trajs = mpd.StopSplitter(splitted_trajs).split(max_diameter=max_diameter,min_duration=min_duration,min_length=min_length)\n",
    "    return splitted_trajs\n",
    "\n",
    "def traj_to_df(trajs):\n",
    "    \"\"\" \n",
    "    Function that transforms a movingpandas trajectory collection into a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trajs: movingpandas trajectory collection\n",
    "    \n",
    "    :returns:  pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    traj_df = pd.DataFrame(trajs.to_line_gdf())\n",
    "    return traj_df\n",
    "\n",
    "def create_splitted_trajectories(df,crs,max_diameter,min_duration,min_length,gap):\n",
    "    \"\"\" \n",
    "    Function that adds trajectory IDs to the pandas dataframe with AIS based on vessel names, time gap, and\n",
    "    stop criteria conditions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    crs: coordinate reference system as a string in a 'EPSG:#'-format\n",
    "    max_diameter: diameter in meters that holds as a boundary box for the stop condition\n",
    "    min_duration: minimum duration as pandas timedelta that holds as a timeframe for the stop condition\n",
    "    min_length: length in meters which a splitted trajectory should have in order to be stored in the collection\n",
    "    gap: time gap in as pandas timedelta\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    import movingpandas as mpd\n",
    "    column_names = df.columns\n",
    "    gdf = create_gdf(df)\n",
    "    transformed_gdf = transform_projection(gdf,crs)\n",
    "    traj_collection = trajectorize(transformed_gdf)\n",
    "    splitted_trajs = splitter(traj_collection,max_diameter,min_duration,min_length,gap)\n",
    "    splitted_trajs.add_traj_id(overwrite=True)\n",
    "    splitted_trajs_df = traj_to_df(splitted_trajs)\n",
    "    splitted_trajs_df = splitted_trajs_df.rename(columns={\"t\": \"timestamplast\"}) \n",
    "    splitted_trajs_df = splitted_trajs_df.reindex(columns=column_names)\n",
    "    return splitted_trajs_df\n",
    "\n",
    "def trajectories_in_areas_of_interest(df,areas_of_interest):\n",
    "    \"\"\" \n",
    "    Function that determines whether a location of an AIS data message is within the areas of interest\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe containing AIS data with longitudes and latitudes in WGS84 in ['longitude'] and \n",
    "        ['latitude'] columns, respectively, and trajectory IDs in a ['traj_id'] column\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe containing AIS data\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_columns = ['name','traj_id','vesseltype','hazardouscargo','length','width','draughtMarine','timestamplast','longitude','latitude','sog','heading','speed','direction','acceleration']\n",
    "    selected_columns.extend([area for area in areas_of_interest])\n",
    "    df_selected = pd.DataFrame(columns = selected_columns)\n",
    "    if len(df) > 2:\n",
    "        gdf = create_gdf(df)\n",
    "        for area in areas_of_interest:\n",
    "            gdf[area] = gdf['geometry'].apply(areas_of_interest[area].intersects)\n",
    "        df_selected = gdf[selected_columns]\n",
    "\n",
    "    return df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b04c91-6684-4892-a551-cb1a434c4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads data\n",
    "ddf = dd.read_parquet(path_name+'/selected_vessels_for_further_analysis', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de990f6a-25e8-4480-91c3-5a640f8181d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performs the functions on the AIS data partitions and saves the data as parquet files in the Azure storage\n",
    "ddf_i = ddf\n",
    "ddf_i = ddf_i.map_partitions(sort_values,['name','timestamplast'])\n",
    "ddf_i = ddf_i.map_partitions(add_columns,added_columns=['direction', 'prev_t', 'speed', 'acceleration', 'traj_id','geometry'])\n",
    "ddf_i = ddf_i.map_partitions(split_trajectories,crs='EPSG:32631',max_diameter=25,min_duration=datetime.timedelta(minutes=30),min_length=0,gap=datetime.timedelta(hours=6),meta=ddf_i)\n",
    "ddf_i['traj_id']=ddf_i['traj_id'].astype(str)\n",
    "ddf_i = ddf_i.map_partitions(trajectories_in_areas_of_interest,areas_of_interest)\n",
    "ddf_i = ddf_i.repartition(partition_size=\"10MB\")\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'traj_id': pa.string(),\n",
    "                      'vesseltype': pa.int64(),\n",
    "                      'hazardouscargo': pa.int64(),\n",
    "                      'length': pa.float64(),\n",
    "                      'width': pa.float64(),\n",
    "                      'draughtMarine': pa.float64(),\n",
    "                      'timestamplast': pa.timestamp('ns', tz='UTC'),\n",
    "                      'longitude': pa.float64(),\n",
    "                      'latitude': pa.float64(),\n",
    "                      'sog': pa.float64(),\n",
    "                      'heading': pa.float64(),\n",
    "                      'speed': pa.float64(),\n",
    "                      'direction': pa.float64(),\n",
    "                      'acceleration': pa.float64(),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow',write_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd81e7a-3f4a-4398-a9dd-0d11e94488c1",
   "metadata": {},
   "source": [
    "### Sort trajectories\n",
    "The trajectories have to be sorted at name and timestamp in order to prepare the data for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a6ce1-025f-410e-8880-6261a1c4d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def reset_index(df):\n",
    "    \"\"\" \n",
    "    Function that resets the index of a pandas dataframe \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=False)\n",
    "    return df\n",
    "\n",
    "def sort_values(df,columns):\n",
    "    \"\"\" \n",
    "    Function that sorts a dataframe based on columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    columns: list of column names\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values(columns)\n",
    "    return df\n",
    "\n",
    "def remame_vessels(df):\n",
    "    \"\"\" \n",
    "    Function that sorts a dataframe based on columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    columns: list of column names\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    names = df['name'].to_list()\n",
    "    for index,name in enumerate(names):\n",
    "        if isinstance(name,str):\n",
    "            names[index] = name.split('_')[0]\n",
    "    df['name'] = names\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb677e-4848-4d69-a8cf-ebb7cfd2ce72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Performs the functions on the AIS data partitions and saves the data as parquet files in the Azure storage\n",
    "cluster.scale(n=5) #Shuffeling of data over partitions must be done using few workers to meet criteria\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(remame_vessels)\n",
    "ddf_i = ddf_i.set_index('name')\n",
    "ddf_i = ddf_i.map_partitions(reset_index)\n",
    "ddf_i = ddf_i.map_partitions(sort_values,['name','timestamplast'])\n",
    "ddf_i.to_parquet(path_name+'/sorted_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "cluster.adapt(minimum=1, maximum=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108437e-c83d-4347-a28a-78fd801db422",
   "metadata": {},
   "source": [
    "### Merging of trajectories that were split by the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb4f8b-6a78-412f-ab45-27cbbf14e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tracks(track1,track2):\n",
    "    \"\"\" \n",
    "    Function that merges two dataframes containing the AIS messages of trajectories\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    track1: pandas dataframe with first trajectory \n",
    "    track2: pandas dataframe with second trajectory  \n",
    "\n",
    "    :returns: movingpandas trajectory \n",
    "    \"\"\"\n",
    "    \n",
    "    track = pd.Series(index=df.iloc[0].keys())\n",
    "    for (index,info1),(index,info2) in zip(track1.items(),track2.items()):\n",
    "        if index in ['name','departure','draught','origin']:\n",
    "            track[index] = info1\n",
    "        elif index in ['arrival','destination']:\n",
    "            track[index] = info2\n",
    "        elif index == 'times':\n",
    "            raw_times = np.append(info1,info2)\n",
    "            track[index] = sorted(raw_times)\n",
    "        elif index in ['coordinates','sog','cog','speed','direction','acceleration']:\n",
    "            info = np.append(info1,info2)\n",
    "            track[index] = sort_together([raw_times,info])[1]\n",
    "        elif index in ['anchorage_areas','port_entrance','berths']:\n",
    "            if True in [info1,info2]:\n",
    "                track[index] = True\n",
    "            else:\n",
    "                track[index] = False\n",
    "    track['geometry'] = LineString(track['coordinates'])\n",
    "    track['distance'] = transform(wgs84_to_utm,track['geometry']).length\n",
    "    track['duration'] = track['arrival']-track['departure']\n",
    "    return track\n",
    "\n",
    "def merge_trajectories(df,gap,distance,utm):\n",
    "    \"\"\" \n",
    "    Function that merges trajectories based on their gap in time and space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    gap: maximum time as a pandas timedelta over which the timestamps of two AIS message of the same vessel \n",
    "         can differ in order to be classified as the same trajectory\n",
    "    distance: maximum distance in meters over which the distance between the locations of two AIS message \n",
    "              of the same vessel can differ in order to be classified as the same trajectory\n",
    "    utm: Universal Transverse Mercator or other EPSG with units in meters as a string in 'EPSG:#'-format \n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df['origin'].iloc[0] == 'foo':\n",
    "        return df\n",
    "    from more_itertools import sort_together\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm = pyproj.CRS(utm)\n",
    "    utm_to_wgs84 = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "    wgs84_to_utm = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "    \n",
    "    merged_trajectories_df = pd.DataFrame(columns=df.columns)\n",
    "    for name in dict.fromkeys(df['name'].to_numpy()).keys():\n",
    "        df_ship = df[df.name == name]\n",
    "        traj_prev = df_ship.iloc[0]\n",
    "        merged_trajectory_df = pd.DataFrame([traj_prev])  \n",
    "        for index_next in df_ship.index[1:]:\n",
    "            traj_next = df_ship.loc[index_next]\n",
    "            end_point = transform(utm_to_wgs84,traj_prev['destination'])\n",
    "            start_point = transform(utm_to_wgs84,traj_next['origin'])\n",
    "            length_prev_traj = traj_prev['distance']\n",
    "            average_speed_prev_traj = np.mean(traj_prev['sog'])\n",
    "            length_next_traj = traj_next['distance']\n",
    "            average_speed_next_traj = np.mean(traj_prev['sog'])\n",
    "            deltatime = traj_next['departure']-traj_prev['arrival']\n",
    "            deltadistance = end_point.distance(start_point)\n",
    "            if deltatime <= pd.Timedelta(0):\n",
    "                traj_prev = merged_trajectory_df.iloc[-1] = merge_tracks(traj_prev,traj_next)\n",
    "            elif deltatime <= gap and deltadistance <= distance and traj_prev['draught'] == traj_next['draught']:\n",
    "                traj_prev = merged_trajectory_df.iloc[-1] = merge_tracks(traj_prev,traj_next)\n",
    "            elif deltatime <= 5*gap and deltadistance <= 5*distance and traj_prev['draught'] == traj_next['draught']:\n",
    "                traj_prev = merged_trajectory_df.iloc[-1] = merge_tracks(traj_prev,traj_next)\n",
    "            else:\n",
    "                merged_trajectory_df = pd.concat([merged_trajectory_df,pd.DataFrame(df_ship.loc[[index_next]])])\n",
    "                traj_prev = merged_trajectory_df.iloc[-1]\n",
    "        merged_trajectories_df = pd.concat([merged_trajectories_df,merged_trajectory_df])\n",
    "    return merged_trajectories_df\n",
    "\n",
    "def create_trajectory_dataframe(df,trajectory_columns,areas_of_interest,utm):\n",
    "    \"\"\" \n",
    "    Function that creates a trajectory dataframe with on each row a separate trajectory \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    trajectory_columns: column names of trajectory dataframe\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "    utm: Universal Transverse Mercator or other EPSG with units in meters as a string in 'EPSG:#'-format \n",
    "\n",
    "    :returns: pandas dataframe with AIS data trajectories\n",
    "    \"\"\"\n",
    "    \n",
    "    import movingpandas as mpd\n",
    "    if len(df) == 0:\n",
    "        return trajectory_dataframe\n",
    "    trajectory_dataframe = pd.DataFrame(columns=trajectory_columns)\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm = pyproj.CRS(utm)\n",
    "    utm_to_wgs84 = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "    \n",
    "    for name in dict.fromkeys(df['name'].to_numpy()).keys():\n",
    "        df_ship = df[df.name == name]\n",
    "        for index,traj_id in enumerate(dict.fromkeys(df_ship['traj_id'].to_numpy()).keys()):\n",
    "            df_traj = df_ship[df_ship.traj_id == traj_id]\n",
    "            df_traj = geopandas.GeoDataFrame(df_traj,columns=df_traj.columns,crs=\"EPSG:4326\",geometry=geopandas.points_from_xy(df_traj.longitude, df_traj.latitude))\n",
    "            df_traj = df_traj.to_crs(utm)\n",
    "            if len(df_traj) <= 1:\n",
    "                continue\n",
    "            trajectory = mpd.Trajectory(df_traj,traj_id='traj_id',t='timestamplast',x='latitude',y='longitude')\n",
    "            if len(trajectory.df) <= 1:\n",
    "                continue\n",
    "            index = index\n",
    "            name = name\n",
    "            departure = trajectory.get_start_time()\n",
    "            arrival = trajectory.get_end_time()\n",
    "            origin = transform(utm_to_wgs84,trajectory.get_start_location())\n",
    "            destination = transform(utm_to_wgs84,trajectory.get_end_location())\n",
    "            distance = trajectory.get_length()\n",
    "            duration = trajectory.get_duration()\n",
    "            draught = trajectory.df['draughtMarine'].mode().to_numpy()\n",
    "            if len(draught) > 0:\n",
    "                draught = draught[0]\n",
    "            else:\n",
    "                draught = np.NaN\n",
    "            geometry = transform(utm_to_wgs84,trajectory.to_linestring())\n",
    "            times = [datetime.datetime.fromtimestamp(time) for time in (trajectory.df.index.to_numpy()-np.datetime64(0,'s'))/np.timedelta64(1, 's')]\n",
    "            coordinates = [transform(utm_to_wgs84,point) for point in trajectory.df['geometry'].to_numpy()]\n",
    "            sog = trajectory.df['sog'].to_numpy()\n",
    "            cog = trajectory.df['heading'].to_numpy()\n",
    "            speed = trajectory.df['speed'].to_numpy()\n",
    "            direction = trajectory.df['direction'].to_numpy()\n",
    "            acceleration = trajectory.df['acceleration'].to_numpy()\n",
    "            anchorage_areas = areas_of_interest['anchorage_areas'].intersects(geometry)\n",
    "            port_entrance = areas_of_interest['port_entrance'].intersects(geometry)\n",
    "            berths = areas_of_interest['berths'].intersects(geometry)\n",
    "            trajectory_dataframe = pd.concat([trajectory_dataframe,pd.DataFrame(data=[[name,departure,arrival,origin,destination,distance,duration,draught,geometry,times,coordinates,sog,cog,speed,direction,acceleration,anchorage_areas,port_entrance,berths]],columns=trajectory_columns,index=[index])])\n",
    "    return trajectory_dataframe\n",
    "\n",
    "def change_data_format(df,data_columns):\n",
    "    \"\"\" \n",
    "    Function that transforms the geometry data into string data in order to save it in dask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with geometries\n",
    "    data_columns: column names of geometries\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    geometries = []\n",
    "    changed_df = pd.DataFrame(columns=df.columns)\n",
    "    for name in list(dict.fromkeys(df.name)):\n",
    "        df_ship = df[df.name == name]\n",
    "        for loc,info in df_ship.iterrows():\n",
    "            for data_column in data_columns:\n",
    "                if type(info[data_column]) == list or type(info[data_column]) == tuple:\n",
    "                    for index,data in enumerate(info[data_column]):\n",
    "                        if not index:\n",
    "                            df_ship.loc[loc,data_column] = [str(data)]\n",
    "                        else:\n",
    "                            df_ship.loc[loc,data_column].append(str(data))\n",
    "                else:\n",
    "                    df_ship.loc[loc,data_column] = str(info[data_column])\n",
    "        changed_df = pd.concat([changed_df,df_ship])\n",
    "    return changed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575cd6f-817b-4a33-a2ea-fb8f98f166bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load data and set the input values\n",
    "ddf = dd.read_parquet(path_name+'/sorted_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "trajectory_columns = ['name','departure','arrival','origin','destination','distance','duration','draught','geometry','times','coordinates','sog','cog','speed','direction','acceleration','anchorage_areas','port_entrance','berths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5d587-4fdf-4952-8887-2f3745086d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Performs the functions on the AIS data partitions and saves the data as parquet files in the Azure storage\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(create_trajectory_dataframe,trajectory_columns=trajectory_columns,areas_of_interest=areas_of_interest,utm='EPSG:32631',meta=trajectory_dataframe)#,areas_of_interest,'EPSG:32631',meta=trajectory_dataframe)\n",
    "ddf_i = ddf_i.map_partitions(merge_trajectories,gap=pd.Timedelta(minutes=1),distance=200,utm='EPSG:32631',meta=trajectory_dataframe)\n",
    "ddf_i = ddf_i.map_partitions(change_data_format,data_columns=['origin','destination','geometry','coordinates'])\n",
    "ddf_i = ddf_i.map_partitions(remame_vessels)\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/merged_sorted_trajectories',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80edef1-b20c-4f27-ad8d-9af89bdb0437",
   "metadata": {},
   "source": [
    "### Isolate idle trajectories\n",
    "Trajectories that were split based on the stop splitter were removed and should be restored for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f8a23-02d7-464c-b191-278694ffab80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "def create_conditions(df,start_date):\n",
    "    \"\"\" \n",
    "    Function that finds the timeframes over which a vessel does not has AIS data that was stored in the\n",
    "    sailing trajectories\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with geometries\n",
    "    start_data: pandas timestamp that indicates the start date of the conditions\n",
    "\n",
    "    :returns: dictionary that contains all the timeframes for each vessel\n",
    "    \"\"\"\n",
    "    \n",
    "    conditions = {}\n",
    "    for name in list(dict.fromkeys(df.name)):\n",
    "        df_ship = df[df.name == name]\n",
    "        conditions[name] = []\n",
    "        for index,info in df_ship.iterrows():\n",
    "            conditions[name].append([start_date,pd.Timestamp(info.departure)]) #,tz='UTC'\n",
    "            start_date = pd.Timestamp(info.arrival) #,tz='UTC'\n",
    "    return conditions\n",
    "    \n",
    "def read_conditions(conditions):\n",
    "    \"\"\" \n",
    "    Function that reads timeframe conditions for each vessel and stores it in a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conditions: dictionary that contains all the timeframes for each vessel\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    read_conditions = pd.DataFrame(columns=['Name','Time_start','Time_stop'])\n",
    "    for dictionary in conditions:\n",
    "        ship_name = list(dictionary.keys())[0]\n",
    "        df = pd.DataFrame(dictionary[ship_name],columns=['Time_start','Time_stop'])\n",
    "        df['Name'] = ship_name\n",
    "        read_conditions = pd.concat([read_conditions,df])\n",
    "    read_conditions = read_conditions.reset_index(drop=True)\n",
    "    return read_conditions\n",
    "    \n",
    "def find_untrajectorized_data(df,condition_df):\n",
    "    \"\"\" \n",
    "    Function that selects the AIS data messages that fall within a conditional timeframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    condition_df: dataframe with the timeframe conditions for each ship\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_columns = list(df.columns)\n",
    "    residual_df = pd.DataFrame(columns=selected_columns)\n",
    "    for name in list(dict.fromkeys(df.name)):\n",
    "        df_ship = df[df.name == name]\n",
    "        if df_ship.empty:\n",
    "            continue\n",
    "        t_start = df_ship['timestamplast'].iloc[0]\n",
    "        t_stop = df_ship['timestamplast'].iloc[-1]\n",
    "        df_conditions = condition_df[((condition_df.Name ==name) & \n",
    "                                      (condition_df.Time_start > t_start) & \n",
    "                                      (condition_df.Time_start < t_stop))]\n",
    "        for _,info in df_conditions.iterrows():\n",
    "            residual_df_ship = df_ship[(df.timestamplast > info.Time_start) & (df.timestamplast < info.Time_stop)]\n",
    "            if len(residual_df_ship):\n",
    "                residual_df_ship['traj_id'] = str(info.Time_start)\n",
    "                residual_df = pd.concat([residual_df,residual_df_ship])\n",
    "    return residual_df\n",
    "\n",
    "def create_idle_trajectories(df,crs):\n",
    "    \"\"\" \n",
    "    Function that creates a trajectory dataframe with on each row a separate trajectory \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    crs: coordinate reference system as a string in a 'EPSG:#'-format\n",
    "\n",
    "    :returns: pandas dataframe with AIS data trajectories\n",
    "    \"\"\"\n",
    "    \n",
    "    import movingpandas as mpd\n",
    "    def create_gdf(df):\n",
    "        gdf = geopandas.GeoDataFrame(df,columns=df.columns,crs=\"EPSG:4326\",geometry=geopandas.points_from_xy(df.longitude, df.latitude))\n",
    "        return gdf\n",
    "    \n",
    "    def transform_projection(gdf,crs):\n",
    "        transformed_gdf = gdf.to_crs(crs)\n",
    "        return transformed_gdf\n",
    "    \n",
    "    def trajectorize(gdf,traj_id):\n",
    "        traj = mpd.Trajectory(gdf,traj_id=traj_id,t='timestamplast',x='latitude',y='longitude')\n",
    "        traj.add_speed(overwrite=True)\n",
    "        traj.add_direction(overwrite=True)\n",
    "        traj.add_acceleration(overwrite=True)\n",
    "        return traj\n",
    "    \n",
    "    def traj_to_df(trajs):\n",
    "        traj_df = pd.DataFrame(trajs.to_line_gdf())\n",
    "        return traj_df\n",
    "    \n",
    "    column_names = df.columns\n",
    "    gdf = create_gdf(df)\n",
    "    transformed_gdf = transform_projection(gdf,crs)\n",
    "    trajectories = []\n",
    "    for traj_id in list(dict.fromkeys(df.traj_id)):\n",
    "        sub_df = transformed_gdf[transformed_gdf.traj_id == traj_id]\n",
    "        if len(sub_df.drop_duplicates('timestamplast')) >= 2:\n",
    "            trajectory = trajectorize(sub_df,traj_id)\n",
    "            trajectories.append(trajectory)\n",
    "            \n",
    "    traj_collection = mpd.TrajectoryCollection(trajectories)\n",
    "    traj_collection_df = pd.DataFrame(columns=column_names)\n",
    "    if traj_collection:\n",
    "        traj_collection_df = traj_to_df(traj_collection)\n",
    "        traj_collection_df = traj_collection_df.rename(columns={\"t\": \"timestamplast\"}) \n",
    "        traj_collection_df = traj_collection_df.reindex(columns=column_names)\n",
    "    return traj_collection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17d0b1-c4c0-48c3-a0e4-d596cd9d4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "ddf = dd.read_parquet(path_name+'/merged_sorted_trajectories',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692abf9-e6ce-4ed8-98e4-e1e8a01cb67e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Performs the functions on the AIS data partitions and compute to get the result\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(create_conditions,start_date=pd.Timestamp('2022-01-01 00:00:00+0000', tz='UTC'))\n",
    "conditions = ddf_i.compute()\n",
    "condition_df = read_conditions(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bac823-6365-4662-856b-2a8d405aa455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "ddf = dd.read_parquet(path_name+'/selected_vessels_for_further_analysis', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd0522-1087-424b-9785-4522b8666b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Performs the functions on the AIS data partitions and saves the data as parquet files in the Azure storage\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(sort_values,['name','timestamplast'])\n",
    "ddf_i = ddf_i.map_partitions(add_columns,added_columns=['direction', 'prev_t', 'speed', 'acceleration', 'traj_id', 'geometry'])\n",
    "ddf_i = ddf_i.map_partitions(find_untrajectorized_data,condition_df=condition_df)\n",
    "ddf_i = ddf_i.map_partitions(create_trajectories,crs='EPSG:32631',meta=ddf_i)\n",
    "ddf_i['traj_id']=ddf_i['traj_id'].astype(str)\n",
    "ddf_i = ddf_i.map_partitions(trajectories_in_areas_of_interest,areas_of_interest)\n",
    "scheme_information = {'vesseltype': pa.int64(),\n",
    "                      'hazardouscargo': pa.int64(),\n",
    "                      'length': pa.float64(),\n",
    "                      'width': pa.float64(),\n",
    "                      'draughtMarine': pa.float64(),\n",
    "                      'timestamplast': pa.timestamp('ns', tz='UTC'),\n",
    "                      'longitude': pa.float64(),\n",
    "                      'latitude': pa.float64(),\n",
    "                      'sog': pa.float64(),\n",
    "                      'heading': pa.float64(),\n",
    "                      'speed': pa.float64(),\n",
    "                      'direction': pa.float64(),\n",
    "                      'acceleration': pa.float64()}\n",
    "for key in areas_of_interest.keys():\n",
    "    scheme_information[key] = pa.bool_()\n",
    "ddf_i.to_parquet(path_name+'/idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},write_index=False,schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b76f25-14cd-4cea-b9a7-2fd5fb4536cf",
   "metadata": {},
   "source": [
    "### Sort idle trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb014010-d9b4-40a4-99bf-d17aa9c30e5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.scale(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dceb3-452e-4e6c-871f-482b6c9280aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.set_index('name')\n",
    "ddf_i = ddf_i.map_partitions(reset_index)\n",
    "ddf_i = ddf_i.map_partitions(sort_values,['name','timestamplast'])\n",
    "ddf_i.to_parquet(path_name+'/sorted_idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "cluster.adapt(minimum=1, maximum=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4eeaa-8262-4097-8e21-bdb1ccd9e0ec",
   "metadata": {},
   "source": [
    "### Merge idle trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e23414-bbcc-48d4-a7ca-82874672120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/sorted_idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6906bc5-b334-4532-89f4-2540668ecb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.partitions[0][ddf.partitions[0].name == 'testschip-10'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef09b3a-0e3f-4d1e-9769-7c474888a101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/sorted_idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(create_trajectory_dataframe,trajectory_dataframe=trajectory_dataframe,areas_of_interest=areas_of_interest,utm='EPSG:32631',meta=trajectory_dataframe)#,areas_of_interest,'EPSG:32631',meta=trajectory_dataframe)\n",
    "ddf_i = ddf_i.map_partitions(merge_trajectories,gap=pd.Timedelta(minutes=1),distance=200,utm='EPSG:32631',meta=trajectory_dataframe)\n",
    "ddf_i = ddf_i.map_partitions(change_data_format,data_columns=['origin','destination','geometry','coordinates'])\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/merged_sorted_idle_trajectories',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd6d14-ccae-42ff-99dd-9d075d5ab2f5",
   "metadata": {},
   "source": [
    "### Merge sailing and idle trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978caad0-1711-43ce-a604-24aaebb5670b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trajectories_ddf = dd.read_parquet(path_name+'/merged_sorted_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "residual_ddf = dd.read_parquet(path_name+'/merged_sorted_idle_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0e8b9-1b08-417d-82f7-5fd77336b406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_ddf = dd.concat([trajectories_ddf,residual_ddf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35a821-ab0b-44d0-8796-37825988be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('ns', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "final_ddf.to_parquet(path_name+'/all_merged_trajectories',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62baadad-6be9-4755-a3cd-71fff401bd12",
   "metadata": {},
   "source": [
    "### Sort trajectories and compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66910874-058e-4078-b12e-066f40a767f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ddf = dd.read_parquet(path_name+'/all_merged_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1fe5e-8c81-4f2d-aac0-4cc325e7451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ddf_i = final_ddf.partitions[:]\n",
    "final_ddf_i = final_ddf_i.map_partitions(reset_index)\n",
    "final_ddf_i = final_ddf_i.map_partitions(sort_values,['name','departure'])\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "final_ddf.to_parquet(path_name+'/all_merged_sorted_trajectories',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1bee7-ec61-48bc-ab2f-b0907984ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/all_merged_sorted_trajectories', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.repartition(npartitions=50)\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/all_merged_sorted_trajectories_comprised', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token}, schema=scheme_information, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b8a75-9207-4323-99c8-70358b757280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
