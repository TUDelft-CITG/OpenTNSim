{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc9bf76-8609-4a89-bfa5-33e6568aebc2",
   "metadata": {},
   "source": [
    "### Import packages and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5282df0-7856-4937-959a-62fdf17b4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_gateway\n",
    "import dask.distributed\n",
    "\n",
    "import dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db086bf6-b6bd-48b8-abed-642eaa10ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon, LineString, Point, MultiPolygon\n",
    "from shapely.ops import transform, cascaded_union\n",
    "import numpy as np\n",
    "import movingpandas as mpd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pyproj\n",
    "import scipy\n",
    "import pyarrow as pa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e5635-67d8-4415-a32f-96d127e2eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the path to load pre-processed ais data\n",
    "folder_name = '2022_PoR'\n",
    "path_name = 'abfs://ais/parquet/' + folder_name  \n",
    "\n",
    "#sets the path to load other local data\n",
    "current_directory = os.getcwd()\n",
    "path = current_directory.split(\"\\\\01_Data_Analysis\\\\02_AIS_data\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2edebbb-b71e-4662-938c-469bdf51de3c",
   "metadata": {},
   "source": [
    "### Loads the access token (we use a SAS-token to protect the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293196f3-d1cf-41c8-812a-7188af375168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for environmental variables for secrets (needs python-dotenv)\n",
    "# You can copy the  .env.example file and rename it to .env (one directory  up from the notebooks)\n",
    "# \n",
    "%load_ext dotenv\n",
    "# Load environment variables from the .env file 1 directory up\n",
    "%dotenv -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e9f1-1fb9-429e-afa4-8b67c306070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the environment variable from the  .env file\n",
    "sas_token = dotenv.dotenv_values()['AZURE_BLOB_SAS_TOKEN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa986180-0e9b-44e5-88f1-1b5bee5dc797",
   "metadata": {},
   "source": [
    "### Creation of the cluster with high worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae2a43-0962-44d7-b813-30ca5a0e6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = dask_gateway.Gateway()\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster_options.worker_memory = 32\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.scale(n=5)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a3591-ac79-47ec-a243-66e1b63ba744",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb45b05-f9e8-4541-89b7-c206162b94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_setup(dask_worker: dask.distributed.Worker):\n",
    "    import os\n",
    "    os.system(\"pip install -q movingpandas\")  # or pip\n",
    "    os.system(\"pip install -q more-itertools\")\n",
    "    os.system(\"pip install -q dask\")\n",
    "\n",
    "client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a42cb4-73f2-4433-96b6-3ea5cad891da",
   "metadata": {},
   "source": [
    "### Geospatial and vessel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d348a48-d43e-400e-95fd-eb6ce51674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utm = pyproj.CRS('EPSG:28992')\n",
    "wgs84 = pyproj.CRS('EPSG:4326')\n",
    "wgs_to_utm = pyproj.Transformer.from_crs(wgs84,utm,always_xy=True).transform\n",
    "utm_to_wgs = pyproj.Transformer.from_crs(utm,wgs84,always_xy=True).transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42537aa8-61db-4667-8ec8-a0613775081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorage_areas = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\anchorage_areas.geojson\")\n",
    "anchorage_areas['geometry'] = [Polygon(geom) for geom in anchorage_areas['geometry']] \n",
    "\n",
    "areas_of_interest = {}\n",
    "areas_of_interest['port_entrance'] = gpd.read_file(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\Port_Entrance.geojson\")['geometry'][0]\n",
    "areas_of_interest['berths'] = transform(utm_to_wgs,MultiPolygon(pickle.load(open(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\berths_PoR.pickle\",'rb'))['geometry'].to_list()))\n",
    "areas_of_interest['anchorage_areas'] = cascaded_union(anchorage_areas['geometry'])\n",
    "\n",
    "anchorage_areas['name'] = anchorage_areas['seamark:name']\n",
    "anchorage_areas = anchorage_areas.set_index('name')\n",
    "anchorage_areas = anchorage_areas[['geometry']]\n",
    "\n",
    "berths = pickle.load(open(path+\"\\\\00_Input_data\\\\01_Geospatial_data\\\\berths_PoR.pickle\",\"rb\"))\n",
    "berths = berths.drop(columns='ZZHVAFKNAAM')\n",
    "\n",
    "for loc,berth_info in berths.iterrows():\n",
    "    berths.loc[loc,'geometry'] = transform(utm_to_wgs,berth_info.geometry.buffer(10))\n",
    "for loc,anchorage_info in anchorage_areas.iterrows():\n",
    "    anchorage_areas.loc[loc,'geometry'] = transform(utm_to_wgs,transform(wgs_to_utm,anchorage_info.geometry).buffer(250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0944d49f-eda2-4cc3-a354-8f3623126483",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/ship_dataframe',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ship_dataframe = ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef04d6-affb-4091-87d2-e4d86c35c515",
   "metadata": {},
   "source": [
    "### Divide partitions based on ship dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562eb181-8fa4-45e4-a818-2f3c5a2fdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/all_merged_sorted_trajectories_comprised', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.set_index('name').repartition(divisions=sorted(list(ship_dataframe.index)))\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/all_merged_sorted_trajectories_indexed', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token}, schema=scheme_information, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c85094-c2c1-4eaf-8d94-4ca4243a9bba",
   "metadata": {},
   "source": [
    "### Closure of old cluster and creation of the cluster with default worker memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34b032-dc7a-4c5c-bc43-a3a6d935c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "gateway = dask_gateway.Gateway()\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.adapt(minimum=1, maximum=100)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f33485-d5b1-4a3c-8234-9fa0fb83047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf317d-82d3-4cb2-a371-99984a6d7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_setup(dask_worker: dask.distributed.Worker):\n",
    "    import os\n",
    "    os.system(\"pip install -q movingpandas\")  # or pip\n",
    "    os.system(\"pip install -q more-itertools\")\n",
    "    os.system(\"pip install -q dask\")\n",
    "\n",
    "client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebbf375-8769-4b32-9f90-5781ab191631",
   "metadata": {},
   "source": [
    "### Repartition of data in equally sized dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52075e9-4444-44ec-9c30-f266fe911847",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/all_merged_sorted_trajectories_indexed',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de020fa6-6565-4df5-a3d3-39ea66b8ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.repartition(partition_size='20MB')\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us', tz='UTC')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/all_merged_sorted_trajectories_indexed_repartitioned',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c4887-2b33-49ab-b13b-1c90ab2ae52f",
   "metadata": {},
   "source": [
    "### Create trip dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8914cb1-0bb8-44de-9da1-85be57b9ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "def drop_index(df):\n",
    "    \"\"\" \n",
    "    Function that deletes the index and resets it with a new order\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=False)\n",
    "    return df\n",
    "    \n",
    "def renumber_index(df):\n",
    "    \"\"\" \n",
    "    Function that sets the index as a column and resets the index with a new order\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def convert_string_geometry_to_shapely_geometry(df,geometry_columns):\n",
    "    \"\"\" \n",
    "    Function that converts string geometry data to shapely geometries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    geometry_columns: columns with geometry types as string data\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty or df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    for column in geometry_columns:\n",
    "        df[column] = df[column].apply(wkt.loads)\n",
    "    return df\n",
    "\n",
    "def correct_ais_signal(df,max_sog,min_lon,max_lon,min_lat,max_lat):\n",
    "    \"\"\" \n",
    "    Function that removes AIS signal that falls outside the area of interest or has a incorrect sog\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    max_sog: maximum allowable speed over ground\n",
    "    min_lon: minimum required longitude\n",
    "    max_lon: maximum allowable longitude\n",
    "    min_lat: minimum required latitude\n",
    "    max_lat: maximum allowable latitude\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    for iloc,(loc,row_info) in enumerate(df.iterrows()):      \n",
    "        new_list = False\n",
    "        for index,(time,point,sog,cog,speed,direction,acceleration) in enumerate(zip(row_info.times,row_info.coordinates,row_info.sog,row_info.cog,row_info.speed,row_info.direction,row_info.acceleration)):\n",
    "            lon,lat = point.x,point.y\n",
    "            if sog < max_sog and min_lon < lon and lon < max_lon and min_lat < lat and lat < max_lat:\n",
    "                if not new_list:\n",
    "                    df.iloc[iloc,df.columns.get_loc('times')] = [time]\n",
    "                    df.iloc[iloc,df.columns.get_loc('coordinates')] = [point]\n",
    "                    df.iloc[iloc,df.columns.get_loc('sog')] = [sog]\n",
    "                    df.iloc[iloc,df.columns.get_loc('cog')] = [cog]\n",
    "                    df.iloc[iloc,df.columns.get_loc('speed')] = [speed]\n",
    "                    df.iloc[iloc,df.columns.get_loc('direction')] = [direction]\n",
    "                    df.iloc[iloc,df.columns.get_loc('acceleration')] = [acceleration]\n",
    "                    new_list = True\n",
    "                else:\n",
    "                    df.iloc[iloc,df.columns.get_loc('times')].append(time)\n",
    "                    df.iloc[iloc,df.columns.get_loc('coordinates')].append(point)\n",
    "                    df.iloc[iloc,df.columns.get_loc('sog')].append(sog)\n",
    "                    df.iloc[iloc,df.columns.get_loc('cog')].append(cog)\n",
    "                    df.iloc[iloc,df.columns.get_loc('speed')].append(speed)\n",
    "                    df.iloc[iloc,df.columns.get_loc('direction')].append(direction)\n",
    "                    df.iloc[iloc,df.columns.get_loc('acceleration')].append(acceleration)      \n",
    "    return df\n",
    "\n",
    "def correct_origin_destination_areas(df,areas_of_interest):\n",
    "    \"\"\" \n",
    "    Function that determines if an origin or destination is within an area of interest\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    for area in areas_of_interest:\n",
    "        for index,info in df.iterrows():\n",
    "            df.loc[index,area] = False\n",
    "            if info.origin.intersects(areas_of_interest[area]) or info.destination.intersects(areas_of_interest[area]):\n",
    "                df.loc[index,area] = True\n",
    "    return df\n",
    "\n",
    "def define_origins_and_destinations(df,areas_of_interest):  \n",
    "    \"\"\" \n",
    "    Function that identifies false positives/negative of the area of interest of origins and destination and\n",
    "    replaces it with a correct one\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    origins = df.origin\n",
    "    destinations = df.destination\n",
    "    for iloc,(loc,row_info) in enumerate(df.iterrows()): \n",
    "        for _,geometries in areas_of_interest.items():\n",
    "            coord_area = []    \n",
    "            for name,geometry in zip(geometries.index,geometries.geometry):\n",
    "                if row_info.origin.intersects(geometry):  \n",
    "                    df.loc[loc,'origin'] = name\n",
    "                if row_info.destination.intersects(geometry): \n",
    "                    df.loc[loc,'destination'] = name\n",
    "    return df\n",
    "\n",
    "def correct_origins_and_destinations_new(df,max_velocity,max_time,max_distance): \n",
    "    \"\"\" \n",
    "    Function that determines if the name of the area of interest in which the origin or destination falls with\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    for index,name in enumerate(list(dict.fromkeys(df.name))):\n",
    "        df_name = df[df.name == name]\n",
    "    \n",
    "        for (loc1,previous_track_info),(loc2,next_track_info) in zip(df_name.iloc[:-1].iterrows(),df_name.iloc[1:].iterrows()):\n",
    "            if previous_track_info.destination == previous_track_info.origin:\n",
    "                continue\n",
    "\n",
    "            #Origin != Destination\n",
    "            if not isinstance(next_track_info.origin,str) and isinstance(previous_track_info.destination,str):\n",
    "                point_origin = next_track_info.origin\n",
    "                point_destination = transform(wgs_to_utm,previous_track_info.coordinates[-1])\n",
    "                distance = point_origin.distance(point_destination)\n",
    "                time = next_track_info.departure-previous_track_info.arrival\n",
    "                velocity = distance/(time/np.timedelta64(1,'s'))\n",
    "\n",
    "                if velocity > max_velocity or distance > max_distance or time > max_time:\n",
    "                    continue\n",
    "\n",
    "                df.loc[loc2,'origin'] = previous_track_info.destination\n",
    "                 \n",
    "            elif not isinstance(previous_track_info.destination,str) and isinstance(next_track_info.origin,str):\n",
    "                point_origin = transform(wgs_to_utm,next_track_info.coordinates[0])\n",
    "                point_destination = previous_track_info.destination \n",
    "                distance = point_origin.distance(point_destination)\n",
    "                time = next_track_info.departure-previous_track_info.arrival\n",
    "                velocity = distance/(time/np.timedelta64(1,'s'))\n",
    "\n",
    "                if velocity > max_velocity or distance > max_distance or time > max_time:\n",
    "                    continue\n",
    "\n",
    "                df.loc[loc1,'destination'] = next_track_info.origin\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def correct_origins_and_destinations(df,areas_of_interest):   \n",
    "    \"\"\" \n",
    "    Function that determines if the name of the area of interest in which the origin or destination falls with\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    knots = 0.5144444444\n",
    "    if df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for index,name in enumerate(list(dict.fromkeys(df.name))):\n",
    "        df_name = df[df.name == name]     \n",
    "        remove_rows = []\n",
    "        for iloc,(loc,row_info) in enumerate(df_name.iterrows()): \n",
    "            if np.mean(row_info.sog) > 4.5:\n",
    "                continue\n",
    "            \n",
    "            for _,geometries in areas_of_interest.items():\n",
    "                coord_area = []    \n",
    "                for coord in row_info.coordinates:\n",
    "                    for name,geometry in zip(geometries.index,geometries.geometry):\n",
    "                        if coord.intersects(geometry):\n",
    "                            coord_area.append(name)\n",
    "\n",
    "            mode_coord_area = []\n",
    "            if coord_area:\n",
    "                mode_coord_area = pd.DataFrame(coord_area).mode().iloc[0][0]\n",
    "            \n",
    "            if len(coord_area) < 0.7*len(row_info.coordinates) and np.mean(row_info.sog) <= 0.3 and loc not in remove_rows:\n",
    "                remove_rows.append(loc)  \n",
    "            \n",
    "            if row_info.origin != row_info.destination and (row_info.duration < pd.Timedelta(minutes = 6) or row_info.distance < 100) and loc not in remove_rows:\n",
    "                remove_rows.append(loc) \n",
    "            \n",
    "            if mode_coord_area and len(coord_area) > 0.7*len(row_info.coordinates):\n",
    "                area = pd.DataFrame(coord_area)[0].mode()[0]\n",
    "                if row_info.origin != row_info.destination and len([coord for coord in coord_area if coord == mode_coord_area]) > 0.5*len(coord_area) and np.mean(row_info.sog) < 0.5 and np.mean(row_info.speed) < 0.5*knots:      \n",
    "                    if iloc-1 >= 0:\n",
    "                        df_name.iloc[iloc-1, df_name.columns.get_loc('destination')] = area\n",
    "                        df_name.iloc[iloc, df_name.columns.get_loc('destination')] = area\n",
    "                        row_info.destination = area\n",
    "                    if iloc+1 <= len(df_name)-1:\n",
    "                        df_name.iloc[iloc+1, df_name.columns.get_loc('origin')] = area\n",
    "                        df_name.iloc[iloc, df_name.columns.get_loc('origin')] = area\n",
    "                        row_info.origin = area\n",
    "                elif row_info.origin == row_info.destination:\n",
    "                    if iloc-1 >= 0:\n",
    "                        df_name.iloc[iloc-1, df_name.columns.get_loc('destination')] = area\n",
    "                    if iloc+1 <= len(df_name)-1:\n",
    "                        df_name.iloc[iloc+1, df_name.columns.get_loc('origin')] = area\n",
    "                    if loc not in remove_rows:\n",
    "                        remove_rows.append(loc)    \n",
    "        \n",
    "            if row_info.origin == row_info.destination and loc not in remove_rows:\n",
    "                remove_rows.append(loc)\n",
    "        \n",
    "        for index in reversed(sorted(remove_rows)):\n",
    "            df_name = df_name.drop(index)\n",
    "        \n",
    "        for iloc,(loc,row_info) in enumerate(df_name.iterrows()):\n",
    "            if iloc == 0:\n",
    "                continue\n",
    "\n",
    "            if row_info.arrival-df_name.iloc[iloc-1].departure > pd.Timedelta(minutes=5):\n",
    "                if iloc+1 <= len(df_name)-1 and type(row_info.destination) == str and row_info.destination != df_name.iloc[iloc+1].origin:\n",
    "                    if row_info.sog[-1] < df_name.iloc[iloc+1].sog[0]:\n",
    "                        df_name.iloc[iloc+1, df_name.columns.get_loc('origin')] = row_info.destination\n",
    "                    else:\n",
    "                        df_name.iloc[iloc, df_name.columns.get_loc('destination')] = df_name.iloc[iloc+1].origin\n",
    "                if iloc-1 >= 0 and type(row_info.origin) == str and row_info.origin != df_name.iloc[iloc-1].destination:\n",
    "                    if row_info.sog[0] < df_name.iloc[iloc-1].sog[-1]:\n",
    "                        df_name.iloc[iloc-1, df_name.columns.get_loc('destination')] = row_info.origin    \n",
    "                    else:\n",
    "                        df_name.iloc[iloc, df_name.columns.get_loc('origin')] = df_name.iloc[iloc-1].destination  \n",
    "                \n",
    "        df_name = df_name.reset_index(drop=True)\n",
    "        new_df = pd.concat([new_df,df_name])\n",
    "    return new_df\n",
    "\n",
    "def create_trips(df,areas_of_interest):\n",
    "    \"\"\" \n",
    "    Function that constructs a trip dataframe based on identificaiton of inbound and outbound trips to and from\n",
    "    and area of interest\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    areas_of_interest: dictionary with areas of interest names as names and shapely polygons as values\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    trip_index = 0\n",
    "    trip_df = pd.DataFrame()\n",
    "    inbound_found = False\n",
    "    for iloc,(loc,row_info) in enumerate(df.iterrows()):\n",
    "        for area in areas_of_interest['berths'].index:    \n",
    "            if row_info.origin == area:\n",
    "                outbound_trip = row_info\n",
    "                index = pd.MultiIndex.from_tuples([(outbound_trip['name'],trip_index)],names=['name','trip_id'])\n",
    "                trip_df = pd.concat([trip_df,pd.DataFrame([outbound_trip[outbound_trip.keys().drop('name')]],index=index)])\n",
    "                trip_index += 1\n",
    "                inbound_found = False\n",
    "                for area in areas_of_interest['berths'].index:\n",
    "                    if row_info.destination == area and not inbound_found:\n",
    "                        inbound_trip = row_info\n",
    "                        index = pd.MultiIndex.from_tuples([(inbound_trip['name'],trip_index)],names=['name','trip_id'])\n",
    "                        trip_df = pd.concat([trip_df,pd.DataFrame([inbound_trip[inbound_trip.keys().drop('name')]],index=index)])\n",
    "                        inbound_found = True\n",
    "                \n",
    "            if row_info.destination == area and not inbound_found:\n",
    "                inbound_trip = row_info\n",
    "                for area in areas_of_interest['anchorage_areas'].index:\n",
    "                    if iloc-1 >= 0 and row_info.origin == area:\n",
    "                        previous_inbound_trip = df.iloc[iloc-1]\n",
    "                        for area in areas_of_interest['anchorage_areas'].index:\n",
    "                            if previous_inbound_trip.destination == area:\n",
    "                                index = pd.MultiIndex.from_tuples([(previous_inbound_trip['name'],trip_index)],names=['name','trip_id'])\n",
    "                                trip_df = pd.concat([trip_df,pd.DataFrame([previous_inbound_trip[previous_inbound_trip.keys().drop('name')]],index=index)])   \n",
    "                                break\n",
    "                index = pd.MultiIndex.from_tuples([(inbound_trip['name'],trip_index)],names=['name','trip_id'])\n",
    "                trip_df = pd.concat([trip_df,pd.DataFrame([inbound_trip[inbound_trip.keys().drop('name')]],index=index)])\n",
    "                inbound_found = True\n",
    "\n",
    "    return trip_df\n",
    "    \n",
    "def change_data_format(df,data_columns):\n",
    "    \"\"\" \n",
    "    Function that transforms the geometry data into string data in order to save it in dask\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with geometries\n",
    "    data_columns: column names of geometries\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for loc,info in df.iterrows():\n",
    "        row_df = pd.DataFrame([info])\n",
    "        for data_column in data_columns:\n",
    "            if type(info[data_column]) == list or type(info[data_column]) == tuple:\n",
    "                for index,data in enumerate(info[data_column]):\n",
    "                    if not index:\n",
    "                        row_df.loc[loc,data_column] = [str(data)]\n",
    "                    else:\n",
    "                        row_df.loc[loc,data_column].append(str(data))\n",
    "            else:\n",
    "                row_df.loc[loc,data_column] = str(info[data_column])\n",
    "        new_df = pd.concat([new_df,row_df])\n",
    "    return new_df\n",
    "\n",
    "def multi_index_to_index(df,columns):\n",
    "    \"\"\" \n",
    "    Function that transforms the multiindex of a dataframe with a singular index\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with geometries\n",
    "    columns: list of a selection of column names\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.reset_index(drop=False)\n",
    "    new_df = pd.DataFrame(columns=columns)\n",
    "    for loc,info in df.iterrows():\n",
    "        df_info = pd.DataFrame([info])\n",
    "        if isinstance(info['index'],tuple):\n",
    "            df_info['name'] = info['index'][0]\n",
    "            df_info['trip_id'] = info['index'][1]\n",
    "        else:\n",
    "            df_info['name'] = info['index']\n",
    "            df_info['trip_id'] = info['index']\n",
    "        new_df = pd.concat([new_df,df_info])\n",
    "    if not df.empty:\n",
    "        new_df = new_df.drop(['index'],axis=1)\n",
    "    new_df['trip_id'] = new_df['trip_id'].astype(int)\n",
    "    return new_df\n",
    "\n",
    "def reorder_columns(df,columns):\n",
    "    \"\"\" \n",
    "    Function that reorder the columns of a dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    columns: list of a selection of ordered column names \n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "def sort_values(df,columns):\n",
    "    \"\"\" \n",
    "    Function that sorts a dataframe based on columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    columns: list of column names\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.sort_values(columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa48603-2c49-4a2c-ae7d-07f85be36dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = pd.DataFrame(columns=['departure','arrival','origin','destination','distance','duration','draught','geometry','times','coordinates','sog','cog','speed','direction','acceleration','anchorage_areas','port_entrance','berths'])\n",
    "ddf = dd.read_parquet(path_name+'/all_merged_sorted_trajectories_indexed_repartitioned',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c17d1a8-4deb-414c-a310-af2b8add3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n=5) #set cluster at 100 workers for more computation power\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(drop_index)\n",
    "ddf_i = ddf_i.map_partitions(renumber_index)\n",
    "ddf_i = ddf_i.map_partitions(sort_values,columns=['name','arrival'])\n",
    "ddf_i = ddf_i.map_partitions(convert_string_geometry_to_shapely_geometry,geometry_columns=['origin','destination','geometry','coordinates'])\n",
    "ddf_i = ddf_i.map_partitions(correct_ais_signal,max_sog=25,min_lon=2.4,max_lon=4.75,min_lat=51.65,max_lat=52.3)\n",
    "ddf_i = ddf_i.map_partitions(correct_origin_destination_areas,areas_of_interest={'berths': areas_of_interest['berths'],'anchorage_areas': areas_of_interest['anchorage_areas']})\n",
    "ddf_i = ddf_i.map_partitions(define_origins_and_destinations,areas_of_interest={'anchorage_areas':anchorage_areas,'berths':berths})\n",
    "ddf_i = ddf_i.map_partitions(correct_origins_and_destinations,areas_of_interest={'anchorage_areas':anchorage_areas,'berths':berths})\n",
    "ddf_i = ddf_i.map_partitions(create_trips,areas_of_interest={'anchorage_areas':anchorage_areas,'berths':berths},meta=trip_df)\n",
    "trip_df_new = pd.DataFrame(columns=['name','trip_id','departure','arrival','origin','destination','distance','duration','draught','geometry','times','coordinates','sog','cog','speed','direction','acceleration','anchorage_areas','port_entrance','berths'])\n",
    "ddf_i = ddf_i.map_partitions(change_data_format,data_columns=['origin','destination','geometry','coordinates'])\n",
    "ddf_i = ddf_i.map_partitions(multi_index_to_index,columns=trip_df_new.columns)\n",
    "ddf_i = ddf_i.map_partitions(reorder_columns,columns=trip_df_new.columns)\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'trip_id': pa.int64(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/trip_dataframe_with_outliers',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4e774-fbad-4c4d-a710-258665ffeab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a32f5c-3698-478e-93b9-6fd553848270",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4c685-9227-459d-9bb0-73ba017f955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_coordinates(df):\n",
    "    \"\"\" \n",
    "    Function that restores errored coordinates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    for loc,trip_info in df.iterrows():    \n",
    "        if trip_info.coordinates[0] == '[':\n",
    "            geom = trip_info.geometry\n",
    "            geom = wkt.loads(geom)\n",
    "            coords = np.empty(0)\n",
    "            for x,y in geom.coords:\n",
    "                coords = np.append(coords,Point(x,y))\n",
    "            coords = coords.astype(str)\n",
    "            df.at[loc,'coordinates'] = list(coords)\n",
    "    return df\n",
    "\n",
    "def outlier_remover(df,ship_dataframe):\n",
    "    \"\"\" \n",
    "    Function that removes outliers of AIS data tracks based on the journal article of Abreu et al. (2021) entitled:\n",
    "    \"A trajectory scoring tool for local anomaly detection in maritime traffic using visual analytics\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    ship_dataframe: pandas dataframe with unique vessel names and their static information\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) and df['origin'].iloc[0] == 'a':\n",
    "        return df\n",
    "    import movingpandas as mpd\n",
    "    knots = 0.514444444\n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm = pyproj.CRS('EPSG:32631')\n",
    "    wgs84_to_utm = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "    utm_to_wgs84 = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for loc,info in df.iterrows():\n",
    "        drop_index = []\n",
    "        filtered_df = pd.DataFrame([info])\n",
    "        for index,coordinates in enumerate(info.coordinates):\n",
    "            index_dropped = True\n",
    "            new_index = index\n",
    "            while index_dropped:  \n",
    "                index_dropped = False\n",
    "                new_index += 1\n",
    "                if new_index == len(info.coordinates):\n",
    "                    continue\n",
    "\n",
    "                if index in drop_index:\n",
    "                    continue        \n",
    "\n",
    "                #Ship parameters\n",
    "                L = ship_dataframe.loc[info['name']]['length']\n",
    "\n",
    "                #Previous point\n",
    "                t_old = info.times[index]\n",
    "                lon_old = info.coordinates[index].x\n",
    "                lat_old = info.coordinates[index].y\n",
    "                sog_old = info.sog[index]*knots\n",
    "                cog_old = info.cog[index]\n",
    "                v_old = info.speed[index]\n",
    "                phi_old = info.direction[index]\n",
    "\n",
    "                #Next point\n",
    "                t_new = info.times[new_index]\n",
    "                lon_new = info.coordinates[new_index].x\n",
    "                lat_new = info.coordinates[new_index].y\n",
    "                sog_new = info.sog[new_index]*knots\n",
    "                cog_new = info.cog[new_index]\n",
    "                v_new = info.speed[new_index]\n",
    "                phi_new = info.direction[new_index]\n",
    "\n",
    "                #Derivations\n",
    "                dlon = lon_new-lon_old\n",
    "                dlat = lat_new-lat_old\n",
    "                dsog = sog_new-sog_old\n",
    "                dcog = (cog_new-cog_old + 180) % 360 - 180\n",
    "                dv = v_new-v_old\n",
    "                dphi = (phi_new-phi_old + 180) % 360 - 180\n",
    "                dt = (t_new-t_old)/np.timedelta64(1, 's')\n",
    "                dist = transform(wgs84_to_utm, LineString([Point(lon_new,lat_new),Point(lon_old,lat_old)])).length\n",
    "                v_mean = np.mean([v_old,v_new])\n",
    "\n",
    "                #Abnormal stop\n",
    "                criteria = [dlon,dlat,dsog,dcog]   \n",
    "                if v_old > 0 and criteria == list(np.zeros(4)):\n",
    "                    drop_index.append(new_index)\n",
    "                    index_dropped = True\n",
    "                    continue\n",
    "\n",
    "                #Acceleration criteria\n",
    "                a = (v_new-v_old)/dt\n",
    "                a_max =  (15*knots)**2/(20*L)\n",
    "                a_min = -(15*knots)**2/(16*L)    \n",
    "                if (a_max-a)*(a-a_min) < 0:\n",
    "                    drop_index.append(new_index)\n",
    "                    index_dropped = True\n",
    "                    continue\n",
    "\n",
    "                #Anomalous drift\n",
    "                s_min = (v_mean-a_min*dt)*dt\n",
    "                s_max = (v_mean+a_max*dt)*dt\n",
    "                s_max = np.max([s_min,s_max])\n",
    "                if dist > s_max:\n",
    "                    drop_index.append(new_index)\n",
    "                    index_dropped = True\n",
    "                    continue\n",
    "\n",
    "                #Anomalous turning point (only for vessels without tugs)\n",
    "                dia = 3*L\n",
    "                if v_old > 4 and dphi/dt > 360*v_old/(np.pi*dia): #use dphi (what is the orientation)\n",
    "                    drop_index.append(new_index)\n",
    "                    index_dropped = True\n",
    "                    continue\n",
    "\n",
    "        new_coordinates = list(info.coordinates)\n",
    "        new_times = list(info.times)\n",
    "        new_sog = list(info.sog)\n",
    "        new_cog = list(info.cog)\n",
    "        for dropped_index in list(reversed(drop_index)):\n",
    "            new_coordinates.pop(dropped_index)\n",
    "            new_times.pop(dropped_index) \n",
    "            new_sog.pop(dropped_index) \n",
    "            new_cog.pop(dropped_index) \n",
    "        filtered_df['coordinates'] = [new_coordinates]\n",
    "        if len(new_coordinates) < 2:\n",
    "            continue\n",
    "        filtered_df['geometry'] = LineString(new_coordinates)\n",
    "        filtered_df['times'] = [new_times]\n",
    "        filtered_df['sog'] = [new_sog]\n",
    "        filtered_df['cog'] = [new_cog]\n",
    "        time_df = pd.DataFrame({'times':filtered_df.times.iloc[0]})\n",
    "        x_df = pd.DataFrame({'x':[coordinate.x for coordinate in filtered_df.coordinates.iloc[0]]})\n",
    "        y_df = pd.DataFrame({'y':[coordinate.y for coordinate in filtered_df.coordinates.iloc[0]]})\n",
    "        trajectory_df = pd.concat([time_df,x_df,y_df],axis=1)\n",
    "        traj = mpd.Trajectory(trajectory_df,traj_id='index',t='times',x='x',y='y')\n",
    "        traj.add_speed()\n",
    "        traj.add_direction()\n",
    "        traj.add_acceleration()\n",
    "        filtered_df['duration'] = traj.get_duration()\n",
    "        filtered_df['distance'] = traj.get_length()\n",
    "        filtered_df['speed'] = [list(traj.df['speed'])]\n",
    "        filtered_df['direction'] = [list(traj.df['direction'])]\n",
    "        filtered_df['acceleration'] = [list(traj.df['acceleration'])]\n",
    "        new_df = pd.concat([new_df,filtered_df])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2bda3-9ad6-4adb-bc02-c90b90507ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(path_name+'/trip_dataframe_with_outliers',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.partitions[:]\n",
    "ddf_i = ddf_i.map_partitions(restore_coordinates)\n",
    "ddf_i = ddf_i.map_partitions(convert_string_geometry_to_shapely_geometry,geometry_columns=['geometry','coordinates'])\n",
    "ddf_i = ddf_i.map_partitions(outlier_remover,ship_dataframe=ship_dataframe)\n",
    "ddf_i = ddf_i.map_partitions(change_data_format,data_columns=['geometry','coordinates'])\n",
    "scheme_information = {'name': pa.string(),\n",
    "                      'trip_id': pa.int64(),\n",
    "                      'departure': pa.timestamp('ns', tz='UTC'),\n",
    "                      'arrival': pa.timestamp('ns', tz='UTC'),\n",
    "                      'origin': pa.string(),\n",
    "                      'destination': pa.string(),\n",
    "                      'distance': pa.float64(),\n",
    "                      'duration': pa.duration('ns'),\n",
    "                      'draught': pa.float64(),\n",
    "                      'geometry': pa.string(),\n",
    "                      'times': pa.list_(pa.timestamp('us')),\n",
    "                      'coordinates': pa.list_(pa.string()),\n",
    "                      'sog': pa.list_(pa.float64()),\n",
    "                      'cog': pa.list_(pa.float64()),\n",
    "                      'speed': pa.list_(pa.float64()),\n",
    "                      'direction': pa.list_(pa.float64()),\n",
    "                      'acceleration': pa.list_(pa.float64()),\n",
    "                      'anchorage_areas': pa.bool_(),\n",
    "                      'port_entrance': pa.bool_(),\n",
    "                      'berths': pa.bool_()}\n",
    "ddf_i.to_parquet(path_name+'/trip_dataframe',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0c036-2f37-4d8d-949f-c5addce4e029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
