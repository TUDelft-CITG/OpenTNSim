{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3705d8a-ed44-4a40-b94c-90176b867645",
   "metadata": {},
   "source": [
    "### Import packages and set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e02df-c322-42af-9aa8-c4f45ba83cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask_gateway\n",
    "import dask.distributed\n",
    "\n",
    "import dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5b8c1-ed95-4cb0-8926-b0897776690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "from shapely.geometry import Polygon, LineString, Point, MultiPolygon\n",
    "from shapely.ops import transform, cascaded_union\n",
    "import numpy as np\n",
    "import movingpandas as mpd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pyproj\n",
    "import scipy\n",
    "import pyarrow as pa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cff0ce0-8491-4c31-af7a-d1397c64a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the path to load pre-processed ais data\n",
    "folder_name = '2022_PoR'\n",
    "path_name = 'abfs://ais/parquet/' + folder_name  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e02d40-efc4-4a2d-ac23-8948d8248659",
   "metadata": {},
   "source": [
    "### Loads the access token (we use a SAS-token to protect the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7944b94-d3cb-4c1f-8a18-f2b153b99bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for environmental variables for secrets (needs python-dotenv)\n",
    "# You can copy the  .env.example file and rename it to .env (one directory  up from the notebooks)\n",
    "# \n",
    "%load_ext dotenv\n",
    "# Load environment variables from the .env file 1 directory up\n",
    "%dotenv -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1975b-822d-4607-9431-a68a9b035ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the environment variable from the  .env file\n",
    "sas_token = dotenv.dotenv_values()['AZURE_BLOB_SAS_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf5e25-87dd-4ae3-8383-90cd7e1f8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = dask_gateway.Gateway()\n",
    "cluster_options = gateway.cluster_options()\n",
    "cluster = gateway.new_cluster(cluster_options)\n",
    "cluster.adapt(minimum=1, maximum=100)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162b9f5-0150-44d6-8280-eb0890ca6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663a326-7735-43b8-9cc3-413f1941b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_setup(dask_worker: dask.distributed.Worker):\n",
    "    import os\n",
    "    os.system(\"pip install -q movingpandas\")  # or pip\n",
    "    os.system(\"pip install -q more-itertools\")\n",
    "    os.system(\"pip install -q dask\")\n",
    "\n",
    "client.register_worker_callbacks(worker_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ac482-f9c5-4fa6-a2c2-0bf912a99dee",
   "metadata": {},
   "source": [
    "### Creates a dataframe with all the vessels of interest and their horizontal dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b3a91-78fe-4d97-9b9d-60eac7bfb9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def create_ship_dataframe(df,static_columns):\n",
    "    \"\"\" \n",
    "    Function that selects the static column information of vessels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    columns: list of the column names of the dataframe that contain static information\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    ship_df = pd.DataFrame(columns=static_columns)\n",
    "    for name in list(dict.fromkeys(df.name)):\n",
    "        df_ship = df[df.name == name]\n",
    "        if len(df_ship):\n",
    "            new_df = pd.DataFrame([],index=[name])\n",
    "            for column in static_columns:\n",
    "                if not column in list(df_ship.columns):\n",
    "                    continue\n",
    "                info = list(df_ship[column].mode())\n",
    "                info.append(0)\n",
    "                new_df[column] = [info[0]]\n",
    "            ship_df = pd.concat([ship_df,new_df])\n",
    "    return ship_df\n",
    "\n",
    "def create_unique_shipdataframe(df):\n",
    "    \"\"\" \n",
    "    Function that creates a dataframe of unique vessels and their horizontal properties\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe with AIS data\n",
    "    columns: list of the column names of the dataframe that contain static information\n",
    "\n",
    "    :returns: pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_ship_df = pd.DataFrame(columns=df.columns)\n",
    "    for name in list(dict.fromkeys(df.index)):\n",
    "        if len(df[df.index == name]):\n",
    "            df_ship = pd.DataFrame([df[df.index == name].iloc[0]],index=[name])\n",
    "            unique_ship_df = pd.concat([unique_ship_df,df_ship])\n",
    "    return unique_ship_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b789b05-1163-470c-9d9d-d9c0cabee877",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_columns = ['vesseltype','hazardouscargo','length','width']\n",
    "ddf = dd.read_parquet(path_name+'/selected_vessels_for_further_analysis', storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token})\n",
    "ddf_i = ddf.partitions[:2]\n",
    "ddf_i = ddf_i.map_partitions(create_ship_dataframe,static_columns=static_columns,meta=pd.DataFrame(columns=static_columns))\n",
    "ddf_i = ddf_i.repartition(npartitions=1)\n",
    "ddf_i = ddf_i.map_partitions(create_unique_shipdataframe)\n",
    "scheme_information = {'vesseltype': pa.int64(),\n",
    "                      'hazardouscargo': pa.int64(),\n",
    "                      'length': pa.float64(),\n",
    "                      'width': pa.float64(),\n",
    "                      '__null_dask_index__': pa.string()}\n",
    "ddf_i.to_parquet(path_name+'/ship_dataframe',storage_options={\"account_name\": \"rwsais\", \"sas_token\": sas_token},schema=scheme_information,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e335f2-1399-4f0b-b1aa-8c3dd8bad648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
