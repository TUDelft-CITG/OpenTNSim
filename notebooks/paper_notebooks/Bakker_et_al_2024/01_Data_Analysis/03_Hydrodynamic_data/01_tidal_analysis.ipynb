{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import copy\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import hatyan\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy as sc\n",
    "import scipy.signal\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Point,Polygon\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pyproj\n",
    "import pytz\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path = path.split(\"\\\\01_Data_Analysis\\\\03_Hydrodynamic_data\")[0]\n",
    "save_path = path+\"\\\\03_Simulation\\\\01_Input_data\\\\02_Hydrodynamic_data\"\n",
    "hydrodynamic_data_file_path = r'C:\\Users\\floorbakker\\Delft University of Technology\\Gijs Hendrickx - OSR-data\\Referentie\\SDS-SALTI_01jan-31dec_2022_his.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309806e0",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6180c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astronomical_tide(signal_time, signal_values, time_zone=None, constituent_list = []):\n",
    "    \"\"\" Function: calculates the astronomical tidal water level\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        signal_time: array containing the timestamps of the series\n",
    "        signal_values: array containing the y-values of the series (water levels or current velocities)\n",
    "        time_zone: time zone of the timetstamps of the signal time\n",
    "        constituent_list: list of constituent strings to be resolved on the water level data (see Hatyan documentation)\n",
    "        \n",
    "        :returns: primary current velocity time series as a numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    #old_stdout = sys.stdout  # backup current stdout\n",
    "    #sys.stdout = open(os.devnull, \"w\")\n",
    "    time_corrections = [datetime.timedelta(seconds=0)]*len(signal_time)\n",
    "    if time_zone:\n",
    "        time_zone = pytz.timezone(time_zone)\n",
    "        times = np.arange(signal_time[0],signal_time[-1]+np.timedelta64(10,'m'),np.timedelta64(10,'m'))\n",
    "        times = [datetime.datetime.fromtimestamp((time-np.datetime64('1970-01-01T00:00:00'))/np.timedelta64(1, 's')) for time in times]\n",
    "        times = [utc_to_local(time,time_zone) for time in times]\n",
    "        times = pd.DatetimeIndex([times])[0]\n",
    "        time_corrections = [t1.replace(tzinfo=None)-t2 for t1,t2 in zip(times,signal_time)]\n",
    "    signal_datetime = [(y- np.datetime64('1970-01-01T00:00:00'))/np.timedelta64(1, 's') for y in signal_time]\n",
    "    duration = signal_datetime[-1] - signal_datetime[0]\n",
    "    if constituent_list == []:\n",
    "        periods = [[datetime.timedelta(minutes=25, hours=12), 'tidalcycle'],\n",
    "                   [datetime.timedelta(days=1), 'day'],\n",
    "                   [datetime.timedelta(days=14), 'springneap'],\n",
    "                   [datetime.timedelta(days=31), 'month'],\n",
    "                   [datetime.timedelta(days=365/2), 'halfyear'],\n",
    "                   [datetime.timedelta(days=365), 'year']]\n",
    "        indexes = [deltatime[0] >= datetime.timedelta(seconds=duration) for deltatime in periods]\n",
    "        if True not in indexes: index = -1\n",
    "        elif datetime.timedelta(seconds=duration) < datetime.timedelta(minutes=25, hours=12): return [signal_time, signal_values]\n",
    "        else: index = indexes.index(True)\n",
    "        const_list = hatyan.get_const_list_hatyan(periods[index][1])\n",
    "    else:\n",
    "        const_list = constituent_list\n",
    "    ts_meas = pd.DataFrame({'values': signal_values}, index=signal_time)\n",
    "    comp_frommeas, comp_allyears = hatyan.get_components_from_ts(ts=ts_meas, const_list=const_list,nodalfactors=True, return_allyears=True,fu_alltimes=True)\n",
    "    ts_prediction = hatyan.prediction(comp=comp_frommeas, nodalfactors=True, xfac=True, fu_alltimes=True,times_ext=signal_time, timestep_min=10)\n",
    "    #sys.stdout = old_stdout  # reset old stdout\n",
    "    return [[index.timestamp()-time_corrections[idx].total_seconds() for idx,index in enumerate(ts_prediction.index)], [value for value in ts_prediction['values']]],time_corrections,comp_frommeas\n",
    "\n",
    "def fixed2bearing(east_velocity, north_velocity, principal_direction):\n",
    "    \"\"\" Function: calculates the velocity components in a reference frame parallel to the principal direction of the\n",
    "                  velocity cluster. It returns the decomposed current velocity magnitudes in a reference frame relative\n",
    "                  to the given principal direction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        east_velocity: velocity in Eastern direction in m/s (float)\n",
    "        north_velocity: velocity in Northern direction in m/s (float)\n",
    "        principal_direction: principal direction in degrees in fixed reference frame North-East (float)\n",
    "        \n",
    "        :returns: primary current velocity time series as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    bearing_rad = np.radians(principal_direction)\n",
    "    x_velocity = np.cos(bearing_rad) * east_velocity + np.sin(bearing_rad) * north_velocity\n",
    "    #y_velocity = -1 * np.sin(bearing_rad) * east_velocity + np.cos(bearing_rad) * north_velocity\n",
    "\n",
    "    return x_velocity\n",
    "\n",
    "def fixed2principal_components(east_velocity_list, north_velocity_list):\n",
    "    \"\"\" Function: calculates the principal components from a cluster of velocities. It returns principal direction\n",
    "                  in degrees (float)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        east_velocity_list: list of velocities in Eastern direction (list)\n",
    "        north_velocity_list: list of velocities in Northern direction (list)\n",
    "        \n",
    "        :returns: principal direction in radians as a float\n",
    "    \"\"\"\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    X = np.column_stack((east_velocity_list, north_velocity_list))\n",
    "    X_pca = pca.fit(X)\n",
    "    y_pca = pca.components_[:, 1][0]\n",
    "    x_pca = pca.components_[:, 0][0]\n",
    "    theta = np.arctan2(y_pca, x_pca)\n",
    "    alpha = np.degrees(theta)\n",
    "\n",
    "    # Correction for positive alpha in coordinate system with positive x-direction upestuary\n",
    "    if alpha >= 0:\n",
    "        alpha = alpha - 180  # in degrees\n",
    "    return alpha\n",
    "\n",
    "def lag_finder(time_signal, signal1, signal2):\n",
    "    \"\"\" Function: calculates the lag between two signals, for example, to calculate the lag between the vertical and \n",
    "                  horizontal tide\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        time_signal: common time-values as an array\n",
    "        signal1: y-values of first signal as an array\n",
    "        signal2: y-values of second signal as an array\n",
    "        \n",
    "        :returns: lag as a time delay\n",
    "    \"\"\"\n",
    "    \n",
    "    #Shifts signals in time to maximize the correlation (signals should have the same timestamps)\n",
    "    nsamples = len(signal2)\n",
    "\n",
    "    #Smooth signal\n",
    "    b, a = sc.signal.butter(2, 0.1)\n",
    "    signal1 = sc.signal.filtfilt(b, a, signal1)\n",
    "    signal2 = sc.signal.filtfilt(b, a, signal2)\n",
    "\n",
    "    #Regularize signals by subtracting mean and dividing by standard deviation\n",
    "    signal2 -= np.mean(signal2);\n",
    "    signal2 /= np.std(signal2)\n",
    "    signal1 -= np.mean(signal1);\n",
    "    signal1 /= np.std(signal1)\n",
    "\n",
    "    #Find cross-correlation\n",
    "    xcorr = sc.signal.correlate(signal2, signal1)\n",
    "\n",
    "    #Create list of tested time shifts\n",
    "    dt = np.arange(1 - nsamples, nsamples)\n",
    "\n",
    "    #Determine time step of the signals\n",
    "    time_step = time_signal[1] - time_signal[0]\n",
    "\n",
    "    #Determine phase shift in seconds (when correlation is maximum): delay of signal1 with respect to signal2 (so if signal1 is shifted with delay, there is no phase shift between signals)\n",
    "    delay = -1 * (dt[xcorr.argmax()] * time_step)\n",
    "\n",
    "    return delay\n",
    "\n",
    "def tidal_periods(dataset,horizontal_tidal_period=False,time_zone=None):\n",
    "    \"\"\" Function: calculates the tidal periods\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: hydrodynamic data in a xarray DataSet, with following columns: 'Primary current velocity', \n",
    "                 'Current velocity', 'STATION', 'Water level', 'TIME'\n",
    "        horizontal_tidal_period: if a horizontal tidal period should be calculated\n",
    "        time_zone: time zone of the timestamps in the dataset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        times_vertical_tidal_periods: array of vertical tidal periods consisting of list of times and tidal period names\n",
    "        times_horizontal_tidal_periods: array of horizontal tidal periods consisting of list of times and tidal period names\n",
    "        phase_lag: phase lags between vertical and horizontal tides\n",
    "        metadata: metadata consisting of the roots (zero-crossings) of the water level data and velocity data, and phase\n",
    "                  lag in radians\n",
    "        astro_water_level_data: astronomic water levels\n",
    "        primary_current_velocity: primary current velocities\n",
    "    \"\"\"\n",
    "    \n",
    "    times_vertical_tidal_periods = []\n",
    "    times_horizontal_tidal_periods = []\n",
    "    observed_times_horizontal_tidal_periods = []\n",
    "    phase_lags = []\n",
    "    primary_current_velocity = []\n",
    "    time_step = (dataset['TIME'].values[1]-dataset['TIME'].values[0])/np.timedelta64(1,'s')\n",
    "    if horizontal_tidal_period:\n",
    "        primary_current_velocity = dataset['Primary current velocity']\n",
    "        depth_averaged_primary_current_velocity = (dataset['Primary current velocity'].transpose('STATION','TIME','LAYER') * layers).sum('LAYER') \n",
    "    metadata = {}\n",
    "    astro_water_level_data = []\n",
    "    \n",
    "    for station in range(len(dataset['STATION'].values)):\n",
    "        roots_wl = []\n",
    "        roots_cv = []\n",
    "        astro_wlev = False\n",
    "        astro_cvel = False\n",
    "        if np.max(dataset['Water level'][station].values) > 0.25:\n",
    "            astro_data_water_level_reduced,_,comp_wlev_red = astronomical_tide(dataset['TIME'].values,dataset['Water level'][station].values,time_zone)\n",
    "            astro_wlev = True\n",
    "            astro_water_level_data.append(astro_data_water_level_reduced[1])\n",
    "        else:\n",
    "            astro_water_level_data.append(np.zeros(len(dataset['TIME'].values)))\n",
    "        \n",
    "        if horizontal_tidal_period and np.max(dataset['Current velocity'][station].values) > 0.25:\n",
    "            astro_data_primary_current_velocity_reduced,_,comp_cur_red = astronomical_tide(dataset['TIME'].values,depth_averaged_primary_current_velocity[station].values,time_zone,reduced_selected_const)   \n",
    "            astro_cvel = True\n",
    "                    \n",
    "        #High low tidal periods\n",
    "        if astro_wlev:\n",
    "            index_prev_root = 0\n",
    "            roots = sc.interpolate.CubicSpline(astro_data_water_level_reduced[0],astro_data_water_level_reduced[1]).roots()\n",
    "            roots_wl = [root for root in roots if root>=astro_data_water_level_reduced[0][0] and root<=astro_data_water_level_reduced[0][-1]]\n",
    "            times_vertical_tidal_period = []\n",
    "            for root in roots_wl:\n",
    "                index_current_root = bisect.bisect_right(astro_data_water_level_reduced[0], root)-1\n",
    "                if index_current_root == -1: \n",
    "                    index_current_root = index_current_root + 1\n",
    "                if len(astro_data_water_level_reduced[1][index_prev_root:index_current_root]) == 0:\n",
    "                    continue\n",
    "                wlev_diff_cross = astro_data_water_level_reduced[1][index_current_root+1]-astro_data_water_level_reduced[1][index_current_root-1]\n",
    "                root = pd.to_datetime(datetime.datetime.fromtimestamp(root,tz=pytz.utc), utc=True).to_datetime64()\n",
    "                if wlev_diff_cross >= 0:\n",
    "                    min_wlev = np.min(astro_data_water_level_reduced[1][index_prev_root:index_current_root])\n",
    "                    times_vertical_tidal_period.append([str(root),'Rising Start'])\n",
    "                    index_prev_root = index_current_root\n",
    "                elif wlev_diff_cross <= 0:\n",
    "                    max_wlev = np.max(astro_data_water_level_reduced[1][index_prev_root:index_current_root])\n",
    "                    times_vertical_tidal_period.append([str(root),'Falling Start'])\n",
    "                    index_prev_root = index_current_root\n",
    "\n",
    "            if times_vertical_tidal_period:\n",
    "                if times_vertical_tidal_period[0][1] == 'Low water Start': \n",
    "                    times_vertical_tidal_period.insert(0,[str(dataset.TIME.values[0]),'High water Start'])\n",
    "                elif times_vertical_tidal_period[0][1] == 'High water Start': \n",
    "                    times_vertical_tidal_period.insert(0,[str(dataset.TIME.values[0]),'Low water Start'])   \n",
    "                if times_vertical_tidal_period[-1][1] == 'Low water Start': \n",
    "                    times_vertical_tidal_period.append([str(dataset.TIME.values[-1]),'High water Start'])\n",
    "                elif times_vertical_tidal_period[-1][1] == 'High water Start': \n",
    "                    times_vertical_tidal_period.append([str(dataset.TIME.values[-1]),'Low water Start']) \n",
    "        \n",
    "        else:\n",
    "            times_vertical_tidal_period = []\n",
    "            \n",
    "        times_vertical_tidal_periods.append(times_vertical_tidal_period)\n",
    "        \n",
    "        \n",
    "        #Rising falling tidal period\n",
    "        if horizontal_tidal_period and astro_wlev:\n",
    "            index_prev_root = 0\n",
    "            roots = sc.interpolate.CubicSpline(astro_data_water_level_reduced[0],astro_data_water_level_reduced[1]).roots()\n",
    "            roots_wl = [root for root in roots if root>=astro_data_water_level_reduced[0][0] and root<=astro_data_water_level_reduced[0][-1]]\n",
    "            times_vertical_tidal_period = []\n",
    "            for root in roots_wl:\n",
    "                index_current_root = bisect.bisect_right(astro_data_water_level_reduced[0], root)-1\n",
    "                if index_current_root == -1: \n",
    "                    index_current_root = index_current_root + 1\n",
    "                if len(astro_data_water_level_reduced[1][index_prev_root:index_current_root]) == 0:\n",
    "                    continue\n",
    "                wlev_diff_cross = astro_data_water_level_reduced[1][index_current_root+1]-astro_data_water_level_reduced[1][index_current_root-1]\n",
    "                root = pd.to_datetime(datetime.datetime.fromtimestamp(root,tz=pytz.utc), utc=True).to_datetime64()\n",
    "                if wlev_diff_cross >= 0:\n",
    "                    min_wlev = np.min(astro_data_water_level_reduced[1][index_prev_root:index_current_root])\n",
    "                    rising_start_index = list(astro_data_water_level_reduced[1][index_prev_root:index_current_root]).index(min_wlev)\n",
    "                    rising_start = astro_data_water_level_reduced[0][index_prev_root:index_current_root][rising_start_index]\n",
    "                    rising_start = pd.to_datetime(datetime.datetime.fromtimestamp(rising_start,tz=pytz.utc), utc=True).to_datetime64()\n",
    "                    times_vertical_tidal_period.append([str(rising_start),'Rising Start'])\n",
    "                    index_prev_root = index_current_root\n",
    "                elif wlev_diff_cross <= 0:\n",
    "                    max_wlev = np.max(astro_data_water_level_reduced[1][index_prev_root:index_current_root])\n",
    "                    falling_start_index = list(astro_data_water_level_reduced[1][index_prev_root:index_current_root]).index(max_wlev)\n",
    "                    falling_start = astro_data_water_level_reduced[0][index_prev_root:index_current_root][falling_start_index]\n",
    "                    falling_start = pd.to_datetime(datetime.datetime.fromtimestamp(falling_start,tz=pytz.utc), utc=True).to_datetime64()\n",
    "                    times_vertical_tidal_period.append([str(falling_start),'Falling Start'])\n",
    "                    index_prev_root = index_current_root\n",
    "\n",
    "            if times_vertical_tidal_period:\n",
    "                if times_vertical_tidal_period[0][1] == 'Falling Start': \n",
    "                    times_vertical_tidal_period.insert(0,[str(dataset.TIME.values[0]),'Rising Start'])\n",
    "                elif times_vertical_tidal_period[0][1] == 'Rising Start': \n",
    "                    times_vertical_tidal_period.insert(0,[str(dataset.TIME.values[0]),'Falling Start'])   \n",
    "                if times_vertical_tidal_period[-1][1] == 'Falling Start': \n",
    "                    times_vertical_tidal_period.append([str(dataset.TIME.values[-1]),'Rising Start'])\n",
    "                elif times_vertical_tidal_period[-1][1] == 'Rising Start': \n",
    "                    times_vertical_tidal_period.append([str(dataset.TIME.values[-1]),'Falling Start']) \n",
    "            times_vertical_tidal_periods.append(times_vertical_tidal_period)\n",
    "\n",
    "        #Phase shift\n",
    "        if horizontal_tidal_period and astro_wlev and astro_cvel:\n",
    "            phase_lag = lag_finder(dataset['TIME'].values,astro_data_water_level_reduced[1],astro_data_primary_current_velocity_reduced[1])\n",
    "            phase_lag = phase_lag.astype(dtype='timedelta64[s]')/np.timedelta64(1,'s')\n",
    "            mean_tidal_period = (dataset['TIME'].values[-1]-dataset['TIME'].values[0])/(len(times_vertical_tidal_period)/2)\n",
    "            mean_tidal_period = mean_tidal_period.astype(dtype='timedelta64[s]')/np.timedelta64(1,'s')\n",
    "        else:\n",
    "            mean_tidal_period = 12.4206012*60*60\n",
    "            phase_lag = 0.25*mean_tidal_period\n",
    "        \n",
    "        #Horizontal tidal period\n",
    "        if horizontal_tidal_period and astro_wlev and astro_cvel or math.isnan(phase_lag):\n",
    "            times_horizontal_tidal_period = []\n",
    "            max_ampl_wlev = np.max(list(comp_wlev_red['A'][0:6]))+np.max(list(comp_wlev_red['A'][6:28]))+np.max(list(comp_wlev_red['A'][28:34]))+np.max(list(comp_wlev_red['A'][34:]))\n",
    "            max_ampl_cur = np.max(list(comp_cur_red['A'][0:6]))+np.max(list(comp_cur_red['A'][6:28]))+np.max(list(comp_cur_red['A'][28:34]))+np.max(list(comp_cur_red['A'][34:]))\n",
    "            mean_tidal_period = 12.4206012*60*60\n",
    "            if max_ampl_cur < 0.4:\n",
    "                for period in times_vertical_tidal_period:\n",
    "                    if period[1] == 'Falling Start':\n",
    "                        times_horizontal_tidal_period.append([period[0],'Ebb Start'])\n",
    "                    elif period[1] == 'Rising Start':\n",
    "                        times_horizontal_tidal_period.append([period[0],'Flood Start'])\n",
    "                phase_lag = 0.25*mean_tidal_period\n",
    "                observed_times_horizontal_tidal_period = times_horizontal_tidal_period\n",
    "            else:\n",
    "                astro_data_primary_current_velocity_reduced[1] = [value-comp_cur_red['A'][0] for value in astro_data_primary_current_velocity_reduced[1]]\n",
    "                for iteration in range(2*int(abs(phase_lag/mean_tidal_period))+1):\n",
    "                    if round(phase_lag/mean_tidal_period,1)*2*np.pi < -0.5*np.pi:\n",
    "                        primary_current_velocity[station] = [-1*value for value in primary_current_velocity[station].values]\n",
    "                        depth_averaged_primary_current_velocity[station] = [-1*value for value in depth_averaged_primary_current_velocity[station].values]\n",
    "                        if phase_lag > 0: phase_lag = 0.5*mean_tidal_period-phase_lag\n",
    "                        elif phase_lag < 0: phase_lag = 0.5*mean_tidal_period+phase_lag\n",
    "                    elif round(phase_lag/mean_tidal_period,1)*2*np.pi > 0.5*np.pi:\n",
    "                        phase_lag = phase_lag-mean_tidal_period\n",
    "                    elif round(phase_lag/mean_tidal_period,1)*2*np.pi < -np.pi:\n",
    "                        phase_lag = phase_lag+mean_tidal_period\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if phase_lag < 0:\n",
    "                    phase_lag = 0\n",
    "\n",
    "                # Interpolation of the variation of the water level and calculation of the zero crossing times\n",
    "                index_prev_root = 0\n",
    "                roots = sc.interpolate.CubicSpline(astro_data_primary_current_velocity_reduced[0],depth_averaged_primary_current_velocity.values[station]).roots()\n",
    "                roots_cv = [root for root in roots if root>=astro_data_primary_current_velocity_reduced[0][0] and root<=astro_data_primary_current_velocity_reduced[0][-1]]\n",
    "                for root in roots_cv:\n",
    "                    index_current_root = bisect.bisect_right(astro_data_water_level_reduced[0], root)-2\n",
    "                    if index_current_root == -1: \n",
    "                        index_current_root = index_current_root + 1\n",
    "                    if len(astro_data_primary_current_velocity_reduced[1][index_prev_root:index_current_root]) == 0:\n",
    "                        continue \n",
    "                    root = pd.to_datetime(datetime.datetime.fromtimestamp(root,tz=pytz.utc), utc=True).to_datetime64()\n",
    "                    cvel_diff_cross = depth_averaged_primary_current_velocity.values[station][index_current_root+1]-depth_averaged_primary_current_velocity.values[station][index_current_root-1]\n",
    "                    if cvel_diff_cross < 0:\n",
    "                        times_horizontal_tidal_period.append([str(root), 'Ebb Start'])\n",
    "                        index_prev_root = index_current_root\n",
    "                    elif cvel_diff_cross > 0:\n",
    "                        times_horizontal_tidal_period.append([str(root), 'Flood Start'])\n",
    "                        index_prev_root = index_current_root\n",
    "                \n",
    "                if times_horizontal_tidal_period:\n",
    "                    if times_horizontal_tidal_period[0][1] == 'Ebb Start': \n",
    "                        times_horizontal_tidal_period.insert(0,[str(dataset.TIME.values[0]),'Flood Start'])\n",
    "                    elif times_horizontal_tidal_period[0][1] == 'Flood Start': \n",
    "                        times_horizontal_tidal_period.insert(0,[str(dataset.TIME.values[0]),'Ebb Start'])   \n",
    "                    if times_horizontal_tidal_period[-1][1] == 'Ebb Start': \n",
    "                        times_horizontal_tidal_period.append([str(dataset.TIME.values[-1]),'Flood Start'])\n",
    "                    elif times_horizontal_tidal_period[-1][1] == 'Flood Start': \n",
    "                        times_horizontal_tidal_period.append([str(dataset.TIME.values[-1]),'Ebb Start']) \n",
    "        \n",
    "        if horizontal_tidal_period:\n",
    "            phase_lags.append(np.degrees(phase_lag/mean_tidal_period*2*np.pi))\n",
    "            times_horizontal_tidal_periods.append(times_horizontal_tidal_period)\n",
    "            metadata[station] = [len(roots_wl),len(roots_cv),np.degrees(phase_lag/mean_tidal_period*2*np.pi)]\n",
    "        \n",
    "    return times_vertical_tidal_periods, times_horizontal_tidal_periods, phase_lags, metadata, astro_water_level_data, primary_current_velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eeba1",
   "metadata": {},
   "source": [
    "Derivation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_vertical_tidal_periods(dataset):\n",
    "    \"\"\" Function: initiates the calculation for the vertical tidal periods periods and adds to the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: hydrodynamic data in a xarray DataSet, with following columns: 'Primary current velocity', \n",
    "                 'Current velocity', 'STATION', 'Water level', 'TIME'\n",
    "        horizontal_tidal_period: if a horizontal tidal period should be calculated\n",
    "        time_zone: time zone of the timestamps in the dataset\n",
    "        \n",
    "        :returns: dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    times_vertical_tidal_periods, times_horizontal_tidal_periods, phase_lags, metadata, astro_wlevs, pcur = tidal_periods(dataset)\n",
    "    dataset['Astronomic water level'] = xr.DataArray(data=astro_wlevs,dims=[\"STATION\",\"TIME\"],coords={'TIME': ('TIME', dataset['TIME'].values), 'STATION': ('STATION', dataset['STATION'].values)})\n",
    "    max_length_tidal_period_data = np.max([len(period) for period in times_vertical_tidal_periods])\n",
    "    tidal_period_data = []\n",
    "    tidal_periods_data = [[np.zeros(2) for j in np.zeros(max_length_tidal_period_data)] for i in range(len(dataset['STATION']))]\n",
    "\n",
    "    for data in tidal_periods_data:\n",
    "        empty_list = []\n",
    "        for value in data:\n",
    "            value[:] = np.nan\n",
    "            empty_list.append(value)\n",
    "        tidal_period_data.append(empty_list)\n",
    "\n",
    "    for period in enumerate(times_vertical_tidal_periods):\n",
    "        for value in enumerate(period[1]):\n",
    "            tidal_periods_data[period[0]][value[0]] = value[1]\n",
    "\n",
    "    times_vertical_tidal_periods = tidal_periods_data\n",
    "    times_vertical_tidal_period_data = xr.DataArray(data=times_vertical_tidal_periods,\n",
    "                                                    dims=[\"STATION\",\"VERTICALTIDES\",\"TIDEINFO\"],\n",
    "                                                    coords={'Stations': ('STATION', dataset['STATION'].values)})\n",
    "\n",
    "    phase_lag_data = xr.DataArray(data=phase_lags,dims=[\"STATION\"])\n",
    "\n",
    "    dataset['Vertical tidal periods'] = times_vertical_tidal_period_data\n",
    "    return dataset\n",
    "\n",
    "def derive_current_velocity_data(dataset):\n",
    "    \"\"\" Function: calculates the current velocity and direction data based on decomposed current velocity data and \n",
    "                  adds to the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: hydrodynamic data in a xarray DataSet, with following columns: 'Easting current velocity' and\n",
    "                 'Northing current velocity'\n",
    "        \n",
    "        :returns: dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset['Current velocity'] = xr.concat([np.sqrt(dataset['Easting current velocity'][index]**2+dataset['Northing current velocity'][index]**2) for index in range(len(dataset['STATION']))],'STATION')\n",
    "    dataset['Current direction'] = xr.concat([np.degrees(np.arctan2(dataset['Easting current velocity'][index],dataset['Northing current velocity'][index])) for index in range(len(dataset['STATION']))],'STATION')\n",
    "    return dataset\n",
    "\n",
    "def derive_primary_current_velocity(dataset):\n",
    "    \"\"\" Function: initates the calculation of the primary current velocity and adds it to the dataset\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: hydrodynamic data in a xarray DataSet, with following columns: 'Easting current velocity' and\n",
    "                 'Northing current velocity'\n",
    "        \n",
    "        :returns: dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    depth_averaged_easting_current_velocity_data = (dataset['Easting current velocity'].transpose('STATION','TIME','LAYER') * layers).sum('LAYER')\n",
    "    depth_averaged_northing_current_velocity_data = (dataset['Northing current velocity'].transpose('STATION','TIME','LAYER') * layers).sum('LAYER')\n",
    "    depth_averaged_principle_components = []\n",
    "    primary_current_velocity = []\n",
    "    for station in range(len(dataset['STATION'].values)):\n",
    "        depth_averaged_principle_components.append(fixed2principal_components(depth_averaged_easting_current_velocity_data[station],\n",
    "                                                                              depth_averaged_northing_current_velocity_data[station]))\n",
    "        \n",
    "        primary_current_velocity.append(fixed2bearing(dataset['Easting current velocity'][station],\n",
    "                                                      dataset['Northing current velocity'][station],\n",
    "                                                      depth_averaged_principle_components[station]))\n",
    "\n",
    "    dataset['Primary current velocity'] = xr.concat(primary_current_velocity,'STATION')\n",
    "    return dataset\n",
    "\n",
    "def derive_tidal_periods(dataset):   \n",
    "    \"\"\" Function: initates the calculation of the horizontal and vertical tidal periods and adds it to the dataset\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: hydrodynamic data in a xarray DataSet, with following columns: 'Easting current velocity' and\n",
    "                 'Northing current velocity'\n",
    "        \n",
    "        :returns: dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    times_vertical_tidal_periods, times_horizontal_tidal_periods, phase_lags, metadata, astro_wlevs, pcur = tidal_periods(dataset)\n",
    "    dataset['Primary current velocity'] = pcur\n",
    "    combined_tidal_periods = [times_vertical_tidal_periods,times_horizontal_tidal_periods]\n",
    "    for repetition in range(2):\n",
    "        max_length_tidal_period_data = np.max([len(period) for period in combined_tidal_periods[repetition]])\n",
    "        tidal_period_data = []\n",
    "        tidal_periods_data = [[np.zeros(2) for j in np.zeros(max_length_tidal_period_data)] for i in range(len(dataset['STATION']))]\n",
    "        \n",
    "        for data in tidal_periods_data:\n",
    "            empty_list = []\n",
    "            for value in data:\n",
    "                value[:] = np.nan\n",
    "                empty_list.append(value)\n",
    "            tidal_period_data.append(empty_list)\n",
    "\n",
    "        for period in enumerate(combined_tidal_periods[repetition]):\n",
    "            for value in enumerate(period[1]):\n",
    "                tidal_periods_data[period[0]][value[0]] = value[1]\n",
    "\n",
    "        combined_tidal_periods[repetition] = tidal_periods_data\n",
    "\n",
    "    times_vertical_tidal_period_data = xr.DataArray(data=combined_tidal_periods[0],\n",
    "                                                    dims=[\"STATION\",\"VERTICALTIDES\",\"TIDEINFO\"],\n",
    "                                                    coords={'Stations': ('STATION', dataset['STATION'].values)})\n",
    "\n",
    "    times_horizontal_tidal_period_data = xr.DataArray(data=combined_tidal_periods[1],\n",
    "                                                      dims=[\"STATION\",\"HORIZONTALTIDES\",\"TIDEINFO\"],\n",
    "                                                      coords={'Stations': ('STATION', dataset['STATION'].values)})\n",
    "\n",
    "    phase_lag_data = xr.DataArray(data=phase_lags,dims=[\"STATION\"])\n",
    "\n",
    "    dataset['Vertical tidal periods'] = times_vertical_tidal_period_data\n",
    "    dataset['Horizontal tidal periods'] = times_horizontal_tidal_period_data\n",
    "    dataset['Phase lag'] = phase_lag_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787fead",
   "metadata": {},
   "source": [
    "## Load port network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"\\\\03_Simulation\\\\01_Input_data\\\\01_Geospatial_data\\\\network\"+\"\\\\PoR_graph_with_information.pickle\", 'rb') as f:\n",
    "    FG = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ff8ac",
   "metadata": {},
   "source": [
    "## Load hydrodynamic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ef66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = xr.open_dataset(hydrodynamic_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd0280",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Change the type of the station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d264ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data['NAMPOL'] = [name.split(' ')[0] for name in hydrodynamic_data['NAMPOL'].values.astype(str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c39ff",
   "metadata": {},
   "source": [
    "Rename coordinates and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360510b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = hydrodynamic_data.rename({'NAMPOL':'STATION'})\n",
    "hydrodynamic_data = hydrodynamic_data.rename({'ZWL':'Water level'})\n",
    "hydrodynamic_data['ZCURU'] = hydrodynamic_data['ZCURU'].rename({'STATIONCUR':'STATION'})\n",
    "hydrodynamic_data = hydrodynamic_data.rename({'ZCURU':'Easting current velocity'})\n",
    "hydrodynamic_data['ZCURV'] = hydrodynamic_data['ZCURV'].rename({'STATIONCUR':'STATION'})\n",
    "hydrodynamic_data = hydrodynamic_data.rename({'ZCURV':'Northing current velocity'})\n",
    "hydrodynamic_data = hydrodynamic_data.rename({'ZKPOLC':'Layer depths'})\n",
    "hydrodynamic_data = hydrodynamic_data.rename({'GRO':'Salinity'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22398928",
   "metadata": {},
   "source": [
    "Add and remove coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0.12, 0.12, 0.11, 0.11, 0.11, 0.11, 0.11, 0.09, 0.06, 0.06]\n",
    "hydrodynamic_data = hydrodynamic_data.assign_coords({'LAYER':layers})\n",
    "hydrodynamic_data['Salinity'] = hydrodynamic_data['Salinity'].squeeze('CONSTIT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5a02b",
   "metadata": {},
   "source": [
    "Add the depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for station_index,_ in enumerate(hydrodynamic_data['STATION']):\n",
    "    data = hydrodynamic_data.isel({'TIME':32000}).isel({'STATION':station_index})\n",
    "    depths.append(np.round(data['Layer depths'].values[-1]-(data['Layer depths'].values[-2]-data['Layer depths'].values[-1])/(layers[-2]/2+layers[-1]/2)*(layers[-1]/2),2))\n",
    "hydrodynamic_data['Depth'] = xr.DataArray(data=depths,coords={'STATION':hydrodynamic_data['STATION']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64535ece",
   "metadata": {},
   "source": [
    "Expand dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path+\"\\\\00_Input_data\\\\02_Hydrodynamic_data\"+'\\\\points_NG04_2023_v103_v3_0.obs','r')\n",
    "stations = {}\n",
    "node_stations = []\n",
    "htide_stations = []\n",
    "for line in text.readlines():\n",
    "    n = int(re.search(r'(?:[, ])(\\d+)\\n', line).group(1))\n",
    "    m = int(re.search(r'(?:[, ]) (\\d+) ', line).group(1))\n",
    "    station = line.split(' ')[0]\n",
    "    stations[station] = (n,m)\n",
    "    if 'P' in line[0]:\n",
    "        node_stations.append((station,stations[station]))\n",
    "    if 'C' in line[0]:\n",
    "        htide_stations.append((station,stations[station]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb36520",
   "metadata": {},
   "outputs": [],
   "source": [
    "surfpath = 'C:\\\\Users\\\\floorbakker\\\\surfdrive\\\\Documents\\\\PhD SALTISolutions\\\\03_Models\\\\OSR2022\\\\'\n",
    "grid = pd.read_csv(surfpath+'NG04_single_2016_v601.csv',delimiter=';',skiprows=2)\n",
    "grid = grid.rename(columns={'0 0 0':'ETA','Unnamed: 1':0,'Unnamed: 2':1,'Unnamed: 3':2,'Unnamed: 4':3,'Unnamed: 5':4,})\n",
    "grid = grid.set_index('ETA')\n",
    "grid.replace(0, np.nan, inplace=True)\n",
    "separation_locs = []\n",
    "for loc,index in enumerate(grid.index):\n",
    "    if 'ETA=' in str(index):\n",
    "        separation_locs.append(loc)\n",
    "separation_locs.append(len(grid))\n",
    "x = []\n",
    "y = []\n",
    "for loc1,loc2 in zip(separation_locs[:-1],separation_locs[1:]):\n",
    "    subgrid = grid.iloc[loc1:loc2]\n",
    "    if loc1 < len(grid)/2:\n",
    "        x.append(subgrid.to_numpy().flatten())\n",
    "    else:\n",
    "        y.append(subgrid.to_numpy().flatten())\n",
    "x = xr.DataArray(x,dims=['N','M'],coords={'N':np.arange(1,len(x)+1,1),'M':np.arange(1,len(x[0])+1,1)})\n",
    "y = xr.DataArray(y,dims=['N','M'],coords={'N':np.arange(1,len(x)+1,1),'M':np.arange(1,len(x[0])+1,1)})  \n",
    "griddata = xr.Dataset()\n",
    "griddata['x'] = x\n",
    "griddata['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ed696",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_nodes = []\n",
    "for node in FG.nodes:\n",
    "    for index,(node_name,(n,m)) in enumerate(node_stations):\n",
    "        if node_name.split('P_')[-1] == node:\n",
    "            break\n",
    "        if index+1 == len(node_stations):\n",
    "            missing_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d664ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "utm = pyproj.CRS('EPSG:28992')\n",
    "wgs84 = pyproj.CRS('EPSG:4326')\n",
    "wgs_to_utm = pyproj.Transformer.from_crs(wgs84,utm,always_xy=True).transform\n",
    "utm_to_wgs = pyproj.Transformer.from_crs(utm,wgs84,always_xy=True).transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 400\n",
    "selected_gridcell = {}\n",
    "for missing_node in missing_nodes:\n",
    "    selected_gridcell[missing_node] = np.NaN\n",
    "    rd_geometry = transform(wgs_to_utm, FG.nodes[missing_node]['geometry'])\n",
    "    subset = griddata.where((griddata.x > rd_geometry.x-offset) & (griddata.x < rd_geometry.x+offset) &\n",
    "                            (griddata.y > rd_geometry.y-offset) & (griddata.y < rd_geometry.y+offset))\n",
    "    stacked_subset_x = subset.x.stack(x=['N','M'])\n",
    "    stacked_subset_y = subset.y.stack(x=['N','M'])\n",
    "    stacked_subset_x_notna = stacked_subset_x[stacked_subset_x.notnull()]\n",
    "    stacked_subset_y_notna = stacked_subset_y[stacked_subset_y.notnull()]\n",
    "    subset_x = stacked_subset_x_notna.unstack()\n",
    "    subset_y = stacked_subset_y_notna.unstack()\n",
    "    subset = xr.Dataset()\n",
    "    subset['x'] = subset_x\n",
    "    subset['y'] = subset_y\n",
    "    subset = subset.sortby(['N','M'])\n",
    "    if not subset.N.values.size or not subset.M.values.size:\n",
    "        continue\n",
    "    \n",
    "    subset_n_values = np.arange(np.min(subset.N.values)-1,np.max(subset.N.values))\n",
    "    subset_m_values = np.arange(np.min(subset.M.values)-1,np.max(subset.M.values))\n",
    "    for n1,n2 in zip(subset_n_values[:-1],subset_n_values[1:]):\n",
    "        for m1,m2 in zip(subset_m_values[:-1],subset_m_values[1:]):\n",
    "            (x1,y1) = (griddata.x.values[n1][m1],griddata.y.values[n1][m1])\n",
    "            (x2,y2) = (griddata.x.values[n1][m2],griddata.y.values[n1][m2])\n",
    "            (x3,y3) = (griddata.x.values[n2][m2],griddata.y.values[n2][m2])\n",
    "            (x4,y4) = (griddata.x.values[n2][m1],griddata.y.values[n2][m1])\n",
    "            if not (True if True in np.isnan(np.array([x1,x2,x3,x4,y1,y2,y3,y4])) else False):\n",
    "                polygon = Polygon([Point(x1,y1),Point(x2,y2),Point(x3,y3),Point(x4,y4)])\n",
    "                if polygon.contains(rd_geometry):\n",
    "                    selected_gridcell[missing_node] = (n2+1,m2+1)\n",
    "                    break\n",
    "    \n",
    "    if not isinstance(selected_gridcell[missing_node],float):\n",
    "        continue\n",
    "    \n",
    "    distance_to_cell = np.NaN*np.ones((len(griddata.N.values),len(griddata.M.values)))\n",
    "    for n1,n2 in zip(subset_n_values[:-1],subset_n_values[1:]):\n",
    "        for m1,m2 in zip(subset_m_values[:-1],subset_m_values[1:]):\n",
    "            (x1,y1) = (griddata.x.values[n1][m1],griddata.y.values[n1][m1])\n",
    "            (x2,y2) = (griddata.x.values[n1][m2],griddata.y.values[n1][m2])\n",
    "            (x3,y3) = (griddata.x.values[n2][m2],griddata.y.values[n2][m2])\n",
    "            (x4,y4) = (griddata.x.values[n2][m1],griddata.y.values[n2][m1])\n",
    "            if not (True if True in np.isnan(np.array([x1,x2,x3,x4,y1,y2,y3,y4])) else False):\n",
    "                distance_to_cell[n2][m2] = Polygon([Point(x1,y1),Point(x2,y2),Point(x3,y3),Point(x4,y4)]).distance(rd_geometry)\n",
    "    \n",
    "    if math.isnan(np.nanmin(distance_to_cell)):\n",
    "        continue\n",
    "    n2,m2 = (np.argwhere(distance_to_cell == np.nanmin(distance_to_cell))[0][0]+1,\n",
    "             np.argwhere(distance_to_cell == np.nanmin(distance_to_cell))[0][1]+1)\n",
    "    selected_gridcell[missing_node] = (n2,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_nodes = {}\n",
    "for missing_node,missing_cell in selected_gridcell.items():\n",
    "    for station,cell in stations.items():\n",
    "        if missing_cell == cell:\n",
    "            replacement_nodes[missing_node] = station\n",
    "            break\n",
    "            \n",
    "    if missing_node not in replacement_nodes.keys():\n",
    "        replacement_nodes[missing_node] = np.NaN\n",
    "replacement_nodes['C_Heysehaven'] = 'P_8861973'\n",
    "replacement_nodes['L_Buitensluis'] = 'P_8861290'\n",
    "replacement_nodes['L_Monstersche_sluis'] = 'P_8865732'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataset = hydrodynamic_data.isel({'STATION':0})\n",
    "dummy_dataset = dummy_dataset.where(lambda x: x == 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb2b57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replacement_dataset = xr.Dataset()\n",
    "for missing_node,replacement_node in replacement_nodes.items():\n",
    "    if isinstance(replacement_nodes[missing_node],str):\n",
    "        replacement_dataset_i = hydrodynamic_data.sel({'STATION':replacement_node})\n",
    "    else:\n",
    "        replacement_dataset_i = dummy_dataset.copy()\n",
    "    replacement_dataset_i = replacement_dataset_i.assign_coords({'STATION':missing_node})\n",
    "    \n",
    "    if not len(replacement_dataset.sizes):\n",
    "        replacement_dataset = replacement_dataset_i\n",
    "    else:\n",
    "        replacement_dataset = xr.concat([replacement_dataset,replacement_dataset_i],dim='STATION')\n",
    "    \n",
    "replacement_dataset = replacement_dataset.transpose('TIME','STATION','LAYER') \n",
    "replacement_dataset['STATION'] = replacement_dataset['STATION'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2740604",
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_names_replacement_data = ['P_'+station if not (station.find('C_')+1 or station.find('L_')+1) else station for station in replacement_dataset.STATION.values]\n",
    "replacement_dataset = replacement_dataset.assign_coords({'STATION':coded_names_replacement_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065755",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_names = list(hydrodynamic_data['STATION'].values)\n",
    "station_names.extend(list(replacement_dataset['STATION'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = hydrodynamic_data.merge(replacement_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a54bef",
   "metadata": {},
   "source": [
    "Rename stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = hydrodynamic_data.reindex({'STATION':station_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c644129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data['TIME'] = hydrodynamic_data['TIME'].where(hydrodynamic_data['TIME'] != -9999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4342aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data.to_netcdf(save_path+'\\\\hydrodynamic_data_preprocessed.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297c846",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db839fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = xr.open_dataset(save_path+'\\\\hydrodynamic_data_preprocessed.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = hatyan.get_full_const_list_withfreqs()\n",
    "const_list = hatyan.get_const_list_hatyan('year')\n",
    "selected_const = []\n",
    "for const in const_list:\n",
    "    index = list(freq_list.index).index(const)\n",
    "    if freq_list['freq'][index] < (1/6):\n",
    "        selected_const.append(const)\n",
    "reduced_selected_const = selected_const[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c40ce",
   "metadata": {},
   "source": [
    "Remove salinity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrodynamic_data = hydrodynamic_data.drop('Salinity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85979f62",
   "metadata": {},
   "source": [
    "#### Water levels\n",
    "Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = hydrodynamic_data.sel({'STATION':[station for station in hydrodynamic_data.STATION.values if station.find('P_')+1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcc801",
   "metadata": {},
   "source": [
    "Replace non-used data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = wlev_hydrodynamic_data.rename({'Easting current velocity':'Current velocity'})\n",
    "wlev_hydrodynamic_data = wlev_hydrodynamic_data.rename({'Northing current velocity':'Current direction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data['Current velocity'] = wlev_hydrodynamic_data['Current velocity'].where(lambda x: x == 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca04cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data['Current direction'] = wlev_hydrodynamic_data['Current velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data['Layer depths'] = wlev_hydrodynamic_data['Layer depths'].where(lambda x: x == 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = wlev_hydrodynamic_data.transpose('STATION','TIME','LAYER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a02339",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = derive_vertical_tidal_periods(wlev_hydrodynamic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data.to_netcdf(save_path+'\\\\wlev_hydrodynamic_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e51f6",
   "metadata": {},
   "source": [
    "### Current velocities\n",
    "Calculate current velocities and directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac33de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = hydrodynamic_data.sel({'STATION':[station for station in hydrodynamic_data.STATION.values if station.find('C_')+1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = current_hydrodynamic_data.transpose('STATION','TIME','LAYER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0.12, 0.12, 0.11, 0.11, 0.11, 0.11, 0.11, 0.09, 0.06, 0.06]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9809491",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = derive_primary_current_velocity(current_hydrodynamic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b666f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = derive_current_velocity_data(current_hydrodynamic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba37391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = derive_tidal_periods(current_hydrodynamic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data = current_hydrodynamic_data.drop('Stations')\n",
    "current_hydrodynamic_data = current_hydrodynamic_data.drop('Easting current velocity')\n",
    "current_hydrodynamic_data = current_hydrodynamic_data.drop('Northing current velocity')\n",
    "current_hydrodynamic_data = current_hydrodynamic_data.drop('Phase lag')\n",
    "final_hydrodynamic_data = final_hydrodynamic_data.drop('Depth')\n",
    "final_hydrodynamic_data = final_hydrodynamic_data.drop('Layer depths')\n",
    "final_hydrodynamic_data = final_hydrodynamic_data.drop('Astronomic water level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hydrodynamic_data.to_netcdf(save_path+'\\\\current_hydrodynamic_data.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386dc439",
   "metadata": {},
   "source": [
    "## Combine water level and current velocity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de29c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = xr.open_dataset(save_path+'\\\\wlev_hydrodynamic_data.nc')\n",
    "current_hydrodynamic_data = xr.open_dataset(save_path+'\\\\current_hydrodynamic_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_variable_names = list(wlev_hydrodynamic_data.variables.keys())\n",
    "current_variable_names = list(current_hydrodynamic_data.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data['Current velocity'] = wlev_hydrodynamic_data['Current velocity'].where(lambda x: x==0,0)\n",
    "wlev_hydrodynamic_data['Current direction'] = wlev_hydrodynamic_data['Current velocity'].copy()\n",
    "missing_variables_wlev_data = list(set(current_variable_names)-set(wlev_hydrodynamic_data))\n",
    "for coord in ['STATION','TIME','LAYER']:\n",
    "    missing_variables_wlev_data.remove(coord)\n",
    "for missing_variable in missing_variables_wlev_data:  \n",
    "    if missing_variable in ['Vertical tidal periods','Horizontal tidal periods']:\n",
    "        if missing_variable == 'Vertical tidal periods':\n",
    "            dim2name = 'VERTICALTIDES'\n",
    "        elif missing_variable == 'Horizontal tidal periods':\n",
    "            dim2name = 'HORIZONTALTIDES'\n",
    "        dummy_dataarray_station = current_hydrodynamic_data[missing_variable].isel({'STATION':[0]}).where(lambda x: x == 'nan','nan')\n",
    "        dummy_dataarray_stations = xr.DataArray([dummy_dataarray_station.values]*316,\n",
    "                                                coords={'dim_0':wlev_hydrodynamic_data.STATION.values})\n",
    "        dummy_dataarray_stations = dummy_dataarray_stations.squeeze('dim_1')\n",
    "        dummy_dataarray_stations = dummy_dataarray_stations.rename({'dim_0':'STATION','dim_2':dim2name,'dim_3':'TIDEINFO'})\n",
    "        wlev_hydrodynamic_data[missing_variable] = dummy_dataarray_stations\n",
    "    else:\n",
    "        wlev_hydrodynamic_data[missing_variable] = wlev_hydrodynamic_data['Current velocity'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_variables_cur_data = list(set(wlev_hydrodynamic_data)-set(current_variable_names))\n",
    "current_hydrodynamic_data['Water level'] = current_hydrodynamic_data['Water level'].where(lambda x: x == 0,0)\n",
    "for missing_variable in missing_variables_cur_data:\n",
    "    current_hydrodynamic_data[missing_variable] = current_hydrodynamic_data['Water level'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37924fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlev_hydrodynamic_data = wlev_hydrodynamic_data.drop('Stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be448898",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax([len(wlev_hydrodynamic_data.VERTICALTIDES.values),len(current_hydrodynamic_data.VERTICALTIDES.values)])\n",
    "max_tides = np.max([len(wlev_hydrodynamic_data.VERTICALTIDES.values),len(current_hydrodynamic_data.VERTICALTIDES.values)])\n",
    "if not index:\n",
    "    dataset = current_hydrodynamic_data\n",
    "else:\n",
    "    dataset = wlev_hydrodynamic_data\n",
    "\n",
    "\n",
    "stations = dataset.STATION.values\n",
    "data = np.zeros((len(stations),max_tides,2))\n",
    "data = np.where(data,'','nan')\n",
    "data = data.astype(dtype='U32')\n",
    "for station_index,station in enumerate(stations):\n",
    "    tidal_data = dataset.sel({'STATION':station})['Vertical tidal periods'].values\n",
    "    for tide in dataset.VERTICALTIDES.values:\n",
    "        data[station_index][tide][0] = tidal_data[tide][0]\n",
    "        data[station_index][tide][1] = tidal_data[tide][1]\n",
    "\n",
    "dataset = dataset.drop('Vertical tidal periods')\n",
    "dataset['Vertical tidal periods'] = xr.DataArray(data,coords={'STATION':stations},dims={'STATION':stations,'VERTICALTIDES':np.arange(0,max_tides),'TIDEINFO':np.arange(0,2)})\n",
    "if not index:\n",
    "    current_hydrodynamic_data = dataset\n",
    "else:\n",
    "    wlev_hydrodynamic_data = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hydrodynamic_data = xr.Dataset()\n",
    "for variable in list(wlev_hydrodynamic_data.variables.keys())[3:]:\n",
    "    merged_variable = xr.concat([wlev_hydrodynamic_data[variable],current_hydrodynamic_data[variable]],dim='STATION')\n",
    "    final_hydrodynamic_data[variable] = merged_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ce4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hydrodynamic_data = final_hydrodynamic_data.transpose('TIME','STATION','LAYER','HORIZONTALTIDES','VERTICALTIDES','TIDEINFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118aecd8",
   "metadata": {},
   "source": [
    "## Rename stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40adda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_station_names = []\n",
    "for station in final_hydrodynamic_data.STATION.values:\n",
    "    if station.find('P_')+1:\n",
    "        final_station_names.append(station.split('P_')[-1])\n",
    "    if station.find('C_')+1:\n",
    "        final_station_names.append(station.split('C_')[-1])\n",
    "        \n",
    "final_hydrodynamic_data = final_hydrodynamic_data.assign_coords({'STATION':final_station_names})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e6e15",
   "metadata": {},
   "source": [
    "## Add MBLs (based on network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c8d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'\\\\geospatial_data'+\"\\\\PoR_graph_with_information.pickle\", 'rb') as f:\n",
    "    FG = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBL_data = np.zeros((len(final_hydrodynamic_data.STATION.values),len(final_hydrodynamic_data.TIME.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for station_index,node in enumerate(final_hydrodynamic_data.STATION.values):\n",
    "    if node in list(FG.nodes):\n",
    "        MBL = FG.nodes[node]['MBL']\n",
    "        MBL_data[station_index][:] = MBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MBL_data = MBL_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hydrodynamic_data['MBL'] = xr.DataArray(MBL_data,{'TIME':final_hydrodynamic_data.TIME.values,'STATION':final_hydrodynamic_data.STATION.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hydrodynamic_data.to_netcdf(save_path+'\\\\hydrodynamic_data_final.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb978962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
